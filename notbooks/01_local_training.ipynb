{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f1af539",
   "metadata": {},
   "source": [
    "# Team7 Assiginment2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e388819",
   "metadata": {},
   "source": [
    "## Step1. Setting and Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0493df46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: . (57477, 9) (3, 4)\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import joblib, os, numpy as np, pandas as pd\n",
    "import torch\n",
    "\n",
    "LOCAL, KAGGLE = \".\", \"/kaggle/input/llm-classification-finetuning\"\n",
    "DATA = LOCAL if os.path.exists(\"./train.csv\") else KAGGLE\n",
    "train = pd.read_csv(f\"{DATA}/train.csv\")\n",
    "test  = pd.read_csv(f\"{DATA}/test.csv\")\n",
    "sample = pd.read_csv(f\"{DATA}/sample_submission.csv\")\n",
    "\n",
    "need = {\"prompt\",\"response_a\",\"response_b\",\"winner_model_a\",\"winner_model_b\",\"winner_tie\"}\n",
    "assert need.issubset(set(train.columns)), f\"column: {need - set(train.columns)} is missing in train.csv\"\n",
    "print(\"DATA:\", DATA, train.shape, test.shape)\n",
    "\n",
    "# target (y)\n",
    "# 0: model_a win, 1: model_b win, 2: tie\n",
    "y = train[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values.argmax(1)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "random_state = 20010815\n",
    "val_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20ce45ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.conda/envs/jun_cls/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All functions loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Global Functions ###\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(random_state)\n",
    "random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_state)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def load_model(candidates, idx=0, device=\"cpu\"):\n",
    "    # Load\n",
    "    last_err = None\n",
    "    path = candidates[idx]\n",
    "    try:\n",
    "        print(\"try:\", path)\n",
    "        model = SentenceTransformer(path, device=device)\n",
    "        print(\"loaded model from:\", path)\n",
    "        return model, path\n",
    "    except Exception as e:\n",
    "        last_err = e\n",
    "    raise RuntimeError(\"Failed to load model. In Kaggle, you need to upload the model folder to Datasets and then link it via 'Add data'. Last error: \" + str(last_err))\n",
    "\n",
    "\n",
    "def build_feat(P, A, B):\n",
    "    AB_diff = A - B\n",
    "    AB_adiff = np.abs(AB_diff)\n",
    "    AB_mul = A * B\n",
    "    PA_mul = P * A\n",
    "    PB_mul = P * B\n",
    "    return np.hstack([P, A, B, AB_diff, AB_adiff, AB_mul, PA_mul, PB_mul])\n",
    "\n",
    "\n",
    "def l2norm(a, eps=1e-12):\n",
    "    n = np.linalg.norm(a, axis=1, keepdims=True)\n",
    "    n = np.clip(n, eps, None)\n",
    "    return a / n\n",
    "\n",
    "def encode_texts(model, texts, batch_size=256):\n",
    "    vecs = []\n",
    "    total_texts = len(texts)\n",
    "    total_batches = (total_texts + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        start_time = time.time()\n",
    "        batch = texts[i:i+batch_size].tolist() if isinstance(texts, pd.Series) else texts[i:i+batch_size]\n",
    "        v = model.encode(batch, batch_size=len(batch), convert_to_numpy=True, normalize_embeddings=False, show_progress_bar=False)\n",
    "        vecs.append(v)\n",
    "\n",
    "        batch_num = (i // batch_size) + 1\n",
    "        print(f\"{batch_num}/{total_batches} | time: {time.time() - start_time:.2f}s\", end='\\r', flush=True)\n",
    "    V = np.vstack(vecs)\n",
    "    return l2norm(V)\n",
    "\n",
    "\n",
    "def create_and_save_submission(predictions, filename, test_df, sample_df):\n",
    "    \"\"\"\n",
    "    Creates a Kaggle submission file from model predictions.\n",
    "    Then, it normalizes the probabilities, performs validation checks, and saves the file.\n",
    "    Args:\n",
    "        predictions (np.array): return value of predict_proba() (N, 3)\n",
    "        filename (str): csv filename to save the submission.\n",
    "        test_df (pd.DataFrame): dataframe containing 'id' column.\n",
    "        sample_df (pd.DataFrame): dataframe to align columns with sample submission.\n",
    "    \"\"\"\n",
    "    print(f\"Creating submission file: {filename}...\")\n",
    "    \n",
    "    # 1. Save Submission File\n",
    "    sub_df = pd.DataFrame({\n",
    "        \"id\": test_df[\"id\"],\n",
    "        \"winner_model_a\": predictions[:, 0],\n",
    "        \"winner_model_b\": predictions[:, 1],\n",
    "        \"winner_tie\":     predictions[:, 2],\n",
    "    })\n",
    "\n",
    "    # 2. Normalization check (safety)\n",
    "    probs = sub_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "    row_sums = probs.sum(axis=1, keepdims=True)\n",
    "    probs = probs / np.clip(row_sums, 1e-15, None)\n",
    "    sub_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]] = probs\n",
    "\n",
    "    # 3. Align columns with sample submission\n",
    "    try:\n",
    "        sub_df = sub_df[sample_df.columns]\n",
    "    except KeyError as e:\n",
    "        print(f\"Warning: Columns in sample_df not found. Saving with default columns. Error: {e}\")\n",
    "\n",
    "    # 4. Save\n",
    "    sub_df.to_csv(filename, index=False)\n",
    "\n",
    "    # 5. Assertions to check file integrity\n",
    "    try:\n",
    "        chk = pd.read_csv(filename)\n",
    "        \n",
    "        assert list(chk.columns) == list(sample_df.columns), \\\n",
    "            f\"Column mismatch. Expected: {list(sample_df.columns)}, Got: {list(chk.columns)}\"\n",
    "        \n",
    "        assert not chk.isna().any().any(), \"NaN values found in submission file.\"\n",
    "        \n",
    "        prob_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "        assert np.allclose(chk[prob_cols].sum(1).values, 1.0), \\\n",
    "            \"Probabilities do not sum to 1.0 for all rows.\"\n",
    "            \n",
    "        print(f\"Successfully saved and verified: {filename} (Shape: {sub_df.shape})\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found after saving: {filename}\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Error: Submission file verification failed! {e}\")\n",
    "    \n",
    "    return sub_df\n",
    "\n",
    "print(\"All functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d809f4a1",
   "metadata": {},
   "source": [
    "### Step 2. Model Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a56282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[skip] all-MiniLM-L6-v2 already exists → ./models/all-MiniLM-L6-v2\n",
      "[skip] all-MiniLM-L12-v2 already exists → ./models/all-MiniLM-L12-v2\n",
      "[skip] e5-small-v2 already exists → ./models/e5-small-v2\n",
      "[skip] e5-base-v2 already exists → ./models/e5-base-v2\n",
      "[skip] e5-large-v2 already exists → ./models/e5-large-v2\n",
      "[skip] sentence-t5-base already exists → ./models/sentence-t5-base\n",
      "[skip] sentence-t5-large already exists → ./models/sentence-t5-large\n",
      "=== Model Download Complete (existing ones skipped) ===\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import os\n",
    "\n",
    "MODELS = {\n",
    "    \"deberta-v3-base\": \"microsoft/deberta-v3-base\",\n",
    "    \"e5-base-v2\":      \"intfloat/e5-base-v2\",\n",
    "}\n",
    "\n",
    "BASE_DIR = \"./models\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "for name, hub_path in MODELS.items():\n",
    "    save_path = os.path.join(BASE_DIR, name)\n",
    "    if os.path.exists(save_path) and os.listdir(save_path):\n",
    "        print(f\"[skip] {name} already exists → {save_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"[download] {name} from {hub_path}\")\n",
    "    try:\n",
    "        # 1) SentenceTransformers \n",
    "        st_model = SentenceTransformer(hub_path)\n",
    "        st_model.save(save_path)\n",
    "        print(f\" -> saved (sentence-transformers) to {save_path}\")\n",
    "        continue\n",
    "    except Exception as e1:\n",
    "        # 2) Hugging Face transformers\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(hub_path)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(hub_path)\n",
    "            tokenizer.save_pretrained(save_path)\n",
    "            model.save_pretrained(save_path)\n",
    "            print(f\" -> saved (transformers) to {save_path}\")\n",
    "        except Exception as e2:\n",
    "            print(f\"[fail] {name}: {e2}\")\n",
    "\n",
    "print(\"=== Model Download Complete (existing ones skipped) ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35351a43",
   "metadata": {},
   "source": [
    "### Candidate 1: DeBERTa + LoRA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6405d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: microsoft/deberta-v3-base\n",
      "LoRA adapter will be saved to: ./models/lora_adapter_deberta-v3-base\n",
      "Training samples: 45981, Validation samples: 11496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.conda/envs/jun_cls/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45981/45981 [00:23<00:00, 1936.88 examples/s]\n",
      "Map: 100%|██████████| 11496/11496 [00:07<00:00, 1638.31 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Candidate 1: DeBERTa + LoRA ###\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType\n",
    ")\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.special import softmax\n",
    "\n",
    "# MODEL_NAME = f\"microsoft/deberta-v3-base\"\n",
    "MODEL_NAME = f\"deberta-v3-base\"\n",
    "max_length = 512 # Max length(tokens) for DeBERTa\n",
    "\n",
    "LORA_ADAPTER_DIR = f\"./models/lora_adapter_{MODEL_NAME.split('/')[-1]}\"\n",
    "\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "print(f\"LoRA adapter will be saved to: {LORA_ADAPTER_DIR}\")\n",
    "\n",
    "train_df_lora = train.copy()\n",
    "train_df_lora['labels'] = y\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df_lora,\n",
    "    test_size=val_size,\n",
    "\n",
    "    stratify=train_df_lora['labels']\n",
    ")\n",
    "print(f\"Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n",
    "\n",
    "# Convert to Hugging Face 'Dataset' object\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Load the tokenizer for our model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "path = f\"./models/{MODEL_NAME}\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, local_files_only=True)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(path, local_files_only=True)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # This formats the input as: [CLS] prompt [SEP] A: response_a [SEP] B: response_b [SEP]\n",
    "    # This is a robust way to present the three pieces of text to the model\n",
    "    \n",
    "    # Combine response_a and response_b into a single string\n",
    "    response_pair = [f\"A: {a} {tokenizer.sep_token} B: {b}\" for a, b in zip(examples['response_a'], examples['response_b'])]\n",
    "    \n",
    "    # Tokenize, using prompt as the first sequence and the combined response as the second\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['prompt'],\n",
    "        response_pair, # This will be the second sequence\n",
    "        max_length=max_length,\n",
    "        truncation=True, # Need to consider whitch option is better\n",
    "        padding=False # DataCollator will handle dynamic padding\n",
    "    )\n",
    "    \n",
    "    # Add labels\n",
    "    tokenized_inputs[\"labels\"] = examples[\"labels\"]\n",
    "    return tokenized_inputs\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_df.columns.tolist())\n",
    "tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True, remove_columns=val_df.columns.tolist())\n",
    "\n",
    "# Data collator will dynamically pad batches to the max length *in that batch*\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "print(\"Data preparation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a504895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter already exists at: ./models/lora_adapter_deberta-v3-base\n",
      "Skipping training and loading existing adapter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1781112/995906066.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded existing LoRA adapter.\n",
      "\n",
      "=== Generating Kaggle Submission ===\n",
      "Tokenizing test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 280.59 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data tokenization complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission file: submission_candidate1_deberta_lora_deberta-v3-base.csv...\n",
      "Successfully saved and verified: submission_candidate1_deberta_lora_deberta-v3-base.csv (Shape: (3, 4))\n",
      "Kaggle submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning with LoRA (NO Quantization)\n",
    "# FINAL SOLUTION: Skip quantization entirely - it has compatibility issues with DeBERTa-v3\n",
    "# Use gradient checkpointing for memory efficiency\n",
    "\n",
    "# Check if LoRA adapter already exists\n",
    "if os.path.exists(LORA_ADAPTER_DIR) and os.path.exists(os.path.join(LORA_ADAPTER_DIR, \"adapter_config.json\")):\n",
    "    print(f\"LoRA adapter already exists at: {LORA_ADAPTER_DIR}\")\n",
    "    print(\"Skipping training and loading existing adapter...\")\n",
    "    \n",
    "    # Load base model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=3,\n",
    "        local_files_only=True\n",
    "    )\n",
    "    \n",
    "    # Load the saved LoRA adapter\n",
    "    from peft import PeftModel\n",
    "    peft_model = PeftModel.from_pretrained(model, LORA_ADAPTER_DIR)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LORA_ADAPTER_DIR)\n",
    "    \n",
    "    # Create a minimal trainer for prediction only\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./{MODEL_NAME.split('/')[-1]}-checkpoints\",\n",
    "        per_device_eval_batch_size=8,\n",
    "        fp16=False,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=peft_model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    print(\"Successfully loaded existing LoRA adapter.\")\n",
    "else:\n",
    "    print(\"No existing LoRA adapter found. Starting training from scratch...\")\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=3,\n",
    "        local_files_only=True\n",
    "    )\n",
    "\n",
    "    # Enable gradient checkpointing to save memory\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\n",
    "            \"query_proj\", \n",
    "            \"key_proj\", \n",
    "            \"value_proj\",\n",
    "            \"dense\"\n",
    "        ],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False\n",
    "    )\n",
    "\n",
    "    # Apply LoRA to the model\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "    peft_model.print_trainable_parameters()\n",
    "\n",
    "    # --- Define Custom Compute Metrics ---\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probs = softmax(logits, axis=1)\n",
    "        \n",
    "        eps = 1e-15\n",
    "        probs = np.clip(probs, eps, 1 - eps)\n",
    "        \n",
    "        loss = log_loss(labels, probs)\n",
    "        return {\"log_loss\": loss}\n",
    "\n",
    "    # --- Define Training Arguments ---\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./{MODEL_NAME.split('/')[-1]}-checkpoints\",\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=4,  # Reduced for memory\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=2,  # Effective batch size = 4*2 = 8\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=200,\n",
    "\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"log_loss\",\n",
    "        greater_is_better=False,\n",
    "\n",
    "        logging_steps=100,\n",
    "        fp16=False,  # Disable fp16 due to gradient checkpointing conflict\n",
    "        report_to=\"none\",\n",
    "        gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    "    )\n",
    "\n",
    "    # --- Initialize Trainer ---\n",
    "    trainer = Trainer(\n",
    "        model=peft_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # --- Start Training ---\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # --- Save the final LoRA adapter ---\n",
    "    # This saves only the small, trainable adapter weights\n",
    "    os.makedirs(LORA_ADAPTER_DIR, exist_ok=True)\n",
    "    trainer.model.save_pretrained(LORA_ADAPTER_DIR)\n",
    "\n",
    "    # Also save the tokenizer\n",
    "    tokenizer.save_pretrained(LORA_ADAPTER_DIR)\n",
    "\n",
    "    print(f\"Training complete. LoRA adapter saved to: {LORA_ADAPTER_DIR}\")\n",
    "\n",
    "# --- Predict on Test Data and Create Kaggle Submission ---\n",
    "print(\"\\n=== Generating Kaggle Submission ===\")\n",
    "\n",
    "# Make predictions on the tokenized test dataset\n",
    "def preprocess_test_function(examples):\n",
    "    # This formats the input as: [CLS] prompt [SEP] A: response_a [SEP] B: response_b [SEP]\n",
    "    \n",
    "    # Combine response_a and response_b into a single string\n",
    "    response_pair = [f\"A: {a} {tokenizer.sep_token} B: {b}\" for a, b in zip(examples['response_a'], examples['response_b'])]\n",
    "    \n",
    "    # Tokenize, using prompt as the first sequence and the combined response as the second\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['prompt'],\n",
    "        response_pair, # This will be the second sequence\n",
    "        max_length=max_length,\n",
    "        truncation=True, # Need to consider whitch option is better\n",
    "        padding=False # DataCollator will handle dynamic padding\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "print(\"Tokenizing test dataset...\")\n",
    "test_dataset = Dataset.from_pandas(test)\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_test_function, batched=True, remove_columns=test.columns.tolist())\n",
    "\n",
    "print(\"Test data tokenization complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bc0fbb",
   "metadata": {},
   "source": [
    "### Candidate 1: Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c63740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Candidate 1 (DeBERTa + LoRA): Calibration ===\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation LogLoss BEFORE Calibration: 1.098774\n",
      "Applying Isotonic calibration...\n",
      "Validation LogLoss AFTER Calibration: 1.073453\n",
      "Improvement: 0.025321\n",
      "\n",
      "Generating calibrated predictions for test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission file: submission_candidate1_deberta_lora_CALIBRATED.csv...\n",
      "Successfully saved and verified: submission_candidate1_deberta_lora_CALIBRATED.csv (Shape: (3, 4))\n",
      "\n",
      "=== Candidate 1 Summary ===\n",
      "Before Calibration - Val LogLoss: 1.098774\n",
      "After Calibration  - Val LogLoss: 1.073453\n",
      "Final submission saved with calibration.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n=== Candidate 1 (DeBERTa + LoRA): Calibration ===\")\n",
    "\n",
    "# Get validation predictions from the existing trainer\n",
    "val_predictions = trainer.predict(tokenized_val_dataset)\n",
    "val_logits = val_predictions.predictions\n",
    "val_probs_before = softmax(val_logits, axis=1)\n",
    "\n",
    "# Get true labels from val_df\n",
    "y_val_c1 = val_df['labels'].values\n",
    "\n",
    "# Calculate log loss BEFORE calibration\n",
    "logloss_before = log_loss(y_val_c1, val_probs_before)\n",
    "print(f\"Validation LogLoss BEFORE Calibration: {logloss_before:.6f}\")\n",
    "\n",
    "# Apply Isotonic Calibration per class\n",
    "# We'll calibrate each class probability separately\n",
    "print(\"Applying Isotonic calibration...\")\n",
    "\n",
    "calibrators = []\n",
    "val_probs_calibrated = np.zeros_like(val_probs_before)\n",
    "\n",
    "for class_idx in range(3):\n",
    "    # Get probabilities for this class\n",
    "    class_probs = val_probs_before[:, class_idx]\n",
    "    \n",
    "    # Create binary labels (1 if true class, 0 otherwise)\n",
    "    y_binary = (y_val_c1 == class_idx).astype(int)\n",
    "    \n",
    "    # Fit isotonic regression\n",
    "    iso = IsotonicRegression(out_of_bounds='clip')\n",
    "    iso.fit(class_probs, y_binary)\n",
    "    \n",
    "    # Calibrate\n",
    "    val_probs_calibrated[:, class_idx] = iso.predict(class_probs)\n",
    "    \n",
    "    calibrators.append(iso)\n",
    "\n",
    "# Normalize probabilities to sum to 1\n",
    "row_sums = val_probs_calibrated.sum(axis=1, keepdims=True)\n",
    "val_probs_calibrated = val_probs_calibrated / np.clip(row_sums, 1e-15, None)\n",
    "\n",
    "# Calculate log loss AFTER calibration\n",
    "logloss_after = log_loss(y_val_c1, val_probs_calibrated)\n",
    "print(f\"Validation LogLoss AFTER Calibration: {logloss_after:.6f}\")\n",
    "print(f\"Improvement: {logloss_before - logloss_after:.6f}\")\n",
    "\n",
    "# Now predict on test set with calibration\n",
    "print(\"\\nGenerating calibrated predictions for test set...\")\n",
    "\n",
    "# Get test predictions\n",
    "test_predictions = trainer.predict(tokenized_test_dataset)\n",
    "test_logits = test_predictions.predictions\n",
    "test_probs_uncalibrated = softmax(test_logits, axis=1)\n",
    "\n",
    "# Apply calibration\n",
    "test_probs_calibrated = np.zeros_like(test_probs_uncalibrated)\n",
    "for class_idx in range(3):\n",
    "    class_probs = test_probs_uncalibrated[:, class_idx]\n",
    "    test_probs_calibrated[:, class_idx] = calibrators[class_idx].predict(class_probs)\n",
    "\n",
    "joblib.dump(calibrators, './models/candidate_1_calibrators.pkl')\n",
    "print(\"Candidate 1 Calibrators SAVED to ./models/candidate_1_calibrators.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9478c2",
   "metadata": {},
   "source": [
    "### Candidate 2: PLM + LightGBM(XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72c4d4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using embedding model: ./models/e5-base-v2\n",
      "try: ./models/e5-base-v2\n",
      "loaded model from: ./models/e5-base-v2\n",
      "Encoding texts\n",
      "Prompt encoding complete.\n",
      "Response A encoding complete.\n",
      "Response B encoding complete.\n",
      "Encoding test data...\n",
      "Test encoding complete.\n",
      "Feature extraction complete. Time taken: 1465.02s\n",
      "Train features shape (X_c2): (57477, 6144)\n",
      "Test features shape (X_test_c2): (3, 6144)\n",
      "Data split into: Train (45981, 6144), Validation (11496, 6144)\n"
     ]
    }
   ],
   "source": [
    "### Candidate 2: PLM + LightGBM(XGBoost) ###\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import joblib # For saving models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Choose the model to run: \"LGBM\" or \"XGBOOST\"\n",
    "\n",
    "MODEL_NAME = \"e5-base-v2\" \n",
    "BEST_EMBEDDING_MODEL_PATH = f\"./models/{MODEL_NAME}\"\n",
    "print(f\"Using embedding model: {BEST_EMBEDDING_MODEL_PATH}\")\n",
    "\n",
    "# Load the chosen embedding model\n",
    "try:\n",
    "    # We pass a list containing only our chosen model path\n",
    "    sbert_model, model_src = load_model([BEST_EMBEDDING_MODEL_PATH], idx=0, device=device)\n",
    "    # print(f\"Successfully loaded model from: {model_src}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model from {BEST_EMBEDDING_MODEL_PATH}. Error: {e}\")\n",
    "\n",
    "print(\"Encoding texts\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Encode training data\n",
    "prompt_emb = encode_texts(sbert_model, train[\"prompt\"])\n",
    "print(\"Prompt encoding complete.\")\n",
    "a_emb = encode_texts(sbert_model, train[\"response_a\"])\n",
    "print(\"Response A encoding complete.\")\n",
    "b_emb = encode_texts(sbert_model, train[\"response_b\"])\n",
    "print(\"Response B encoding complete.\")\n",
    "\n",
    "# Build training features\n",
    "X_c2 = build_feat(prompt_emb, a_emb, b_emb) # X for candidate 2\n",
    "\n",
    "# Encode test data\n",
    "print(\"Encoding test data...\")\n",
    "prompt_emb_te = encode_texts(sbert_model, test[\"prompt\"])\n",
    "a_emb_te = encode_texts(sbert_model, test[\"response_a\"])\n",
    "b_emb_te = encode_texts(sbert_model, test[\"response_b\"])\n",
    "print(\"Test encoding complete.\")\n",
    "\n",
    "# Build test features\n",
    "X_test_c2 = build_feat(prompt_emb_te, a_emb_te, b_emb_te)\n",
    "\n",
    "# Clean up model from memory\n",
    "del sbert_model, prompt_emb, a_emb, b_emb, prompt_emb_te, a_emb_te, b_emb_te\n",
    "\n",
    "print(f\"Feature extraction complete. Time taken: {time.time() - start_time:.2f}s\")\n",
    "print(f\"Train features shape (X_c2): {X_c2.shape}\")\n",
    "print(f\"Test features shape (X_test_c2): {X_test_c2.shape}\")\n",
    "\n",
    "# Create Train/Validation Split\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X_c2, y, test_size=val_size, stratify=y, random_state=random_state)\n",
    "print(f\"Data split into: Train {X_tr.shape}, Validation {X_va.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43264750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Candidate 2: XGBOOST ---\n",
      "Model will be saved to: ./models/candidate_2_xgboost_e5-base-v2.pkl\n",
      "[0]\tvalidation_0-mlogloss:1.09635\n",
      "[1]\tvalidation_0-mlogloss:1.09427\n",
      "[2]\tvalidation_0-mlogloss:1.09219\n",
      "[3]\tvalidation_0-mlogloss:1.09043\n",
      "[4]\tvalidation_0-mlogloss:1.08872\n",
      "[5]\tvalidation_0-mlogloss:1.08706\n",
      "[6]\tvalidation_0-mlogloss:1.08541\n",
      "[7]\tvalidation_0-mlogloss:1.08398\n",
      "[8]\tvalidation_0-mlogloss:1.08247\n",
      "[9]\tvalidation_0-mlogloss:1.08107\n",
      "[10]\tvalidation_0-mlogloss:1.07967\n",
      "[11]\tvalidation_0-mlogloss:1.07837\n",
      "[12]\tvalidation_0-mlogloss:1.07707\n",
      "[13]\tvalidation_0-mlogloss:1.07584\n",
      "[14]\tvalidation_0-mlogloss:1.07469\n",
      "[15]\tvalidation_0-mlogloss:1.07350\n",
      "[16]\tvalidation_0-mlogloss:1.07261\n",
      "[17]\tvalidation_0-mlogloss:1.07167\n",
      "[18]\tvalidation_0-mlogloss:1.07075\n",
      "[19]\tvalidation_0-mlogloss:1.06978\n",
      "[20]\tvalidation_0-mlogloss:1.06879\n",
      "[21]\tvalidation_0-mlogloss:1.06788\n",
      "[22]\tvalidation_0-mlogloss:1.06699\n",
      "[23]\tvalidation_0-mlogloss:1.06616\n",
      "[24]\tvalidation_0-mlogloss:1.06532\n",
      "[25]\tvalidation_0-mlogloss:1.06454\n",
      "[26]\tvalidation_0-mlogloss:1.06380\n",
      "[27]\tvalidation_0-mlogloss:1.06300\n",
      "[28]\tvalidation_0-mlogloss:1.06233\n",
      "[29]\tvalidation_0-mlogloss:1.06138\n",
      "[30]\tvalidation_0-mlogloss:1.06071\n",
      "[31]\tvalidation_0-mlogloss:1.06004\n",
      "[32]\tvalidation_0-mlogloss:1.05930\n",
      "[33]\tvalidation_0-mlogloss:1.05868\n",
      "[34]\tvalidation_0-mlogloss:1.05811\n",
      "[35]\tvalidation_0-mlogloss:1.05760\n",
      "[36]\tvalidation_0-mlogloss:1.05713\n",
      "[37]\tvalidation_0-mlogloss:1.05655\n",
      "[38]\tvalidation_0-mlogloss:1.05600\n",
      "[39]\tvalidation_0-mlogloss:1.05553\n",
      "[40]\tvalidation_0-mlogloss:1.05501\n",
      "[41]\tvalidation_0-mlogloss:1.05454\n",
      "[42]\tvalidation_0-mlogloss:1.05419\n",
      "[43]\tvalidation_0-mlogloss:1.05374\n",
      "[44]\tvalidation_0-mlogloss:1.05318\n",
      "[45]\tvalidation_0-mlogloss:1.05289\n",
      "[46]\tvalidation_0-mlogloss:1.05246\n",
      "[47]\tvalidation_0-mlogloss:1.05213\n",
      "[48]\tvalidation_0-mlogloss:1.05187\n",
      "[49]\tvalidation_0-mlogloss:1.05153\n",
      "[50]\tvalidation_0-mlogloss:1.05114\n",
      "[51]\tvalidation_0-mlogloss:1.05072\n",
      "[52]\tvalidation_0-mlogloss:1.05041\n",
      "[53]\tvalidation_0-mlogloss:1.05010\n",
      "[54]\tvalidation_0-mlogloss:1.04964\n",
      "[55]\tvalidation_0-mlogloss:1.04945\n",
      "[56]\tvalidation_0-mlogloss:1.04920\n",
      "[57]\tvalidation_0-mlogloss:1.04896\n",
      "[58]\tvalidation_0-mlogloss:1.04863\n",
      "[59]\tvalidation_0-mlogloss:1.04832\n",
      "[60]\tvalidation_0-mlogloss:1.04802\n",
      "[61]\tvalidation_0-mlogloss:1.04785\n",
      "[62]\tvalidation_0-mlogloss:1.04766\n",
      "[63]\tvalidation_0-mlogloss:1.04737\n",
      "[64]\tvalidation_0-mlogloss:1.04706\n",
      "[65]\tvalidation_0-mlogloss:1.04679\n",
      "[66]\tvalidation_0-mlogloss:1.04653\n",
      "[67]\tvalidation_0-mlogloss:1.04624\n",
      "[68]\tvalidation_0-mlogloss:1.04594\n",
      "[69]\tvalidation_0-mlogloss:1.04572\n",
      "[70]\tvalidation_0-mlogloss:1.04556\n",
      "[71]\tvalidation_0-mlogloss:1.04541\n",
      "[72]\tvalidation_0-mlogloss:1.04509\n",
      "[73]\tvalidation_0-mlogloss:1.04493\n",
      "[74]\tvalidation_0-mlogloss:1.04481\n",
      "[75]\tvalidation_0-mlogloss:1.04462\n",
      "[76]\tvalidation_0-mlogloss:1.04437\n",
      "[77]\tvalidation_0-mlogloss:1.04423\n",
      "[78]\tvalidation_0-mlogloss:1.04405\n",
      "[79]\tvalidation_0-mlogloss:1.04383\n",
      "[80]\tvalidation_0-mlogloss:1.04363\n",
      "[81]\tvalidation_0-mlogloss:1.04362\n",
      "[82]\tvalidation_0-mlogloss:1.04355\n",
      "[83]\tvalidation_0-mlogloss:1.04334\n",
      "[84]\tvalidation_0-mlogloss:1.04322\n",
      "[85]\tvalidation_0-mlogloss:1.04301\n",
      "[86]\tvalidation_0-mlogloss:1.04292\n",
      "[87]\tvalidation_0-mlogloss:1.04276\n",
      "[88]\tvalidation_0-mlogloss:1.04256\n",
      "[89]\tvalidation_0-mlogloss:1.04242\n",
      "[90]\tvalidation_0-mlogloss:1.04235\n",
      "[91]\tvalidation_0-mlogloss:1.04221\n",
      "[92]\tvalidation_0-mlogloss:1.04202\n",
      "[93]\tvalidation_0-mlogloss:1.04182\n",
      "[94]\tvalidation_0-mlogloss:1.04175\n",
      "[95]\tvalidation_0-mlogloss:1.04168\n",
      "[96]\tvalidation_0-mlogloss:1.04160\n",
      "[97]\tvalidation_0-mlogloss:1.04146\n",
      "[98]\tvalidation_0-mlogloss:1.04132\n",
      "[99]\tvalidation_0-mlogloss:1.04122\n",
      "[100]\tvalidation_0-mlogloss:1.04104\n",
      "[101]\tvalidation_0-mlogloss:1.04092\n",
      "[102]\tvalidation_0-mlogloss:1.04085\n",
      "[103]\tvalidation_0-mlogloss:1.04067\n",
      "[104]\tvalidation_0-mlogloss:1.04070\n",
      "[105]\tvalidation_0-mlogloss:1.04050\n",
      "[106]\tvalidation_0-mlogloss:1.04044\n",
      "[107]\tvalidation_0-mlogloss:1.04034\n",
      "[108]\tvalidation_0-mlogloss:1.04011\n",
      "[109]\tvalidation_0-mlogloss:1.04007\n",
      "[110]\tvalidation_0-mlogloss:1.04004\n",
      "[111]\tvalidation_0-mlogloss:1.03998\n",
      "[112]\tvalidation_0-mlogloss:1.03987\n",
      "[113]\tvalidation_0-mlogloss:1.03973\n",
      "[114]\tvalidation_0-mlogloss:1.03969\n",
      "[115]\tvalidation_0-mlogloss:1.03953\n",
      "[116]\tvalidation_0-mlogloss:1.03936\n",
      "[117]\tvalidation_0-mlogloss:1.03924\n",
      "[118]\tvalidation_0-mlogloss:1.03925\n",
      "[119]\tvalidation_0-mlogloss:1.03915\n",
      "[120]\tvalidation_0-mlogloss:1.03904\n",
      "[121]\tvalidation_0-mlogloss:1.03908\n",
      "[122]\tvalidation_0-mlogloss:1.03896\n",
      "[123]\tvalidation_0-mlogloss:1.03889\n",
      "[124]\tvalidation_0-mlogloss:1.03877\n",
      "[125]\tvalidation_0-mlogloss:1.03867\n",
      "[126]\tvalidation_0-mlogloss:1.03857\n",
      "[127]\tvalidation_0-mlogloss:1.03858\n",
      "[128]\tvalidation_0-mlogloss:1.03844\n",
      "[129]\tvalidation_0-mlogloss:1.03844\n",
      "[130]\tvalidation_0-mlogloss:1.03842\n",
      "[131]\tvalidation_0-mlogloss:1.03838\n",
      "[132]\tvalidation_0-mlogloss:1.03822\n",
      "[133]\tvalidation_0-mlogloss:1.03815\n",
      "[134]\tvalidation_0-mlogloss:1.03810\n",
      "[135]\tvalidation_0-mlogloss:1.03804\n",
      "[136]\tvalidation_0-mlogloss:1.03793\n",
      "[137]\tvalidation_0-mlogloss:1.03797\n",
      "[138]\tvalidation_0-mlogloss:1.03795\n",
      "[139]\tvalidation_0-mlogloss:1.03787\n",
      "[140]\tvalidation_0-mlogloss:1.03786\n",
      "[141]\tvalidation_0-mlogloss:1.03778\n",
      "[142]\tvalidation_0-mlogloss:1.03762\n",
      "[143]\tvalidation_0-mlogloss:1.03766\n",
      "[144]\tvalidation_0-mlogloss:1.03761\n",
      "[145]\tvalidation_0-mlogloss:1.03751\n",
      "[146]\tvalidation_0-mlogloss:1.03753\n",
      "[147]\tvalidation_0-mlogloss:1.03747\n",
      "[148]\tvalidation_0-mlogloss:1.03747\n",
      "[149]\tvalidation_0-mlogloss:1.03755\n",
      "[150]\tvalidation_0-mlogloss:1.03757\n",
      "[151]\tvalidation_0-mlogloss:1.03751\n",
      "[152]\tvalidation_0-mlogloss:1.03753\n",
      "[153]\tvalidation_0-mlogloss:1.03751\n",
      "[154]\tvalidation_0-mlogloss:1.03751\n",
      "[155]\tvalidation_0-mlogloss:1.03751\n",
      "[156]\tvalidation_0-mlogloss:1.03749\n",
      "[157]\tvalidation_0-mlogloss:1.03745\n",
      "[158]\tvalidation_0-mlogloss:1.03729\n",
      "[159]\tvalidation_0-mlogloss:1.03722\n",
      "[160]\tvalidation_0-mlogloss:1.03711\n",
      "[161]\tvalidation_0-mlogloss:1.03700\n",
      "[162]\tvalidation_0-mlogloss:1.03695\n",
      "[163]\tvalidation_0-mlogloss:1.03699\n",
      "[164]\tvalidation_0-mlogloss:1.03692\n",
      "[165]\tvalidation_0-mlogloss:1.03691\n",
      "[166]\tvalidation_0-mlogloss:1.03683\n",
      "[167]\tvalidation_0-mlogloss:1.03674\n",
      "[168]\tvalidation_0-mlogloss:1.03681\n",
      "[169]\tvalidation_0-mlogloss:1.03680\n",
      "[170]\tvalidation_0-mlogloss:1.03682\n",
      "[171]\tvalidation_0-mlogloss:1.03681\n",
      "[172]\tvalidation_0-mlogloss:1.03683\n",
      "[173]\tvalidation_0-mlogloss:1.03681\n",
      "[174]\tvalidation_0-mlogloss:1.03673\n",
      "[175]\tvalidation_0-mlogloss:1.03673\n",
      "[176]\tvalidation_0-mlogloss:1.03681\n",
      "[177]\tvalidation_0-mlogloss:1.03680\n",
      "[178]\tvalidation_0-mlogloss:1.03683\n",
      "[179]\tvalidation_0-mlogloss:1.03682\n",
      "[180]\tvalidation_0-mlogloss:1.03679\n",
      "[181]\tvalidation_0-mlogloss:1.03680\n",
      "[182]\tvalidation_0-mlogloss:1.03681\n",
      "[183]\tvalidation_0-mlogloss:1.03675\n",
      "[184]\tvalidation_0-mlogloss:1.03679\n",
      "[185]\tvalidation_0-mlogloss:1.03671\n",
      "[186]\tvalidation_0-mlogloss:1.03671\n",
      "[187]\tvalidation_0-mlogloss:1.03672\n",
      "[188]\tvalidation_0-mlogloss:1.03675\n",
      "[189]\tvalidation_0-mlogloss:1.03672\n",
      "[190]\tvalidation_0-mlogloss:1.03660\n",
      "[191]\tvalidation_0-mlogloss:1.03658\n",
      "[192]\tvalidation_0-mlogloss:1.03659\n",
      "[193]\tvalidation_0-mlogloss:1.03659\n",
      "[194]\tvalidation_0-mlogloss:1.03658\n",
      "[195]\tvalidation_0-mlogloss:1.03656\n",
      "[196]\tvalidation_0-mlogloss:1.03652\n",
      "[197]\tvalidation_0-mlogloss:1.03657\n",
      "[198]\tvalidation_0-mlogloss:1.03649\n",
      "[199]\tvalidation_0-mlogloss:1.03640\n",
      "[200]\tvalidation_0-mlogloss:1.03642\n",
      "[201]\tvalidation_0-mlogloss:1.03648\n",
      "[202]\tvalidation_0-mlogloss:1.03641\n",
      "[203]\tvalidation_0-mlogloss:1.03643\n",
      "[204]\tvalidation_0-mlogloss:1.03643\n",
      "[205]\tvalidation_0-mlogloss:1.03637\n",
      "[206]\tvalidation_0-mlogloss:1.03632\n",
      "[207]\tvalidation_0-mlogloss:1.03633\n",
      "[208]\tvalidation_0-mlogloss:1.03639\n",
      "[209]\tvalidation_0-mlogloss:1.03638\n",
      "[210]\tvalidation_0-mlogloss:1.03636\n",
      "[211]\tvalidation_0-mlogloss:1.03637\n",
      "[212]\tvalidation_0-mlogloss:1.03638\n",
      "[213]\tvalidation_0-mlogloss:1.03647\n",
      "[214]\tvalidation_0-mlogloss:1.03648\n",
      "[215]\tvalidation_0-mlogloss:1.03643\n",
      "[216]\tvalidation_0-mlogloss:1.03639\n",
      "[217]\tvalidation_0-mlogloss:1.03637\n",
      "[218]\tvalidation_0-mlogloss:1.03640\n",
      "[219]\tvalidation_0-mlogloss:1.03641\n",
      "[220]\tvalidation_0-mlogloss:1.03644\n",
      "[221]\tvalidation_0-mlogloss:1.03648\n",
      "[222]\tvalidation_0-mlogloss:1.03640\n",
      "[223]\tvalidation_0-mlogloss:1.03638\n",
      "[224]\tvalidation_0-mlogloss:1.03639\n",
      "[225]\tvalidation_0-mlogloss:1.03643\n",
      "[226]\tvalidation_0-mlogloss:1.03639\n",
      "[227]\tvalidation_0-mlogloss:1.03640\n",
      "[228]\tvalidation_0-mlogloss:1.03633\n",
      "[229]\tvalidation_0-mlogloss:1.03631\n",
      "[230]\tvalidation_0-mlogloss:1.03627\n",
      "[231]\tvalidation_0-mlogloss:1.03629\n",
      "[232]\tvalidation_0-mlogloss:1.03626\n",
      "[233]\tvalidation_0-mlogloss:1.03631\n",
      "[234]\tvalidation_0-mlogloss:1.03627\n",
      "[235]\tvalidation_0-mlogloss:1.03621\n",
      "[236]\tvalidation_0-mlogloss:1.03628\n",
      "[237]\tvalidation_0-mlogloss:1.03628\n",
      "[238]\tvalidation_0-mlogloss:1.03616\n",
      "[239]\tvalidation_0-mlogloss:1.03616\n",
      "[240]\tvalidation_0-mlogloss:1.03613\n",
      "[241]\tvalidation_0-mlogloss:1.03616\n",
      "[242]\tvalidation_0-mlogloss:1.03617\n",
      "[243]\tvalidation_0-mlogloss:1.03621\n",
      "[244]\tvalidation_0-mlogloss:1.03618\n",
      "[245]\tvalidation_0-mlogloss:1.03617\n",
      "[246]\tvalidation_0-mlogloss:1.03615\n",
      "[247]\tvalidation_0-mlogloss:1.03620\n",
      "[248]\tvalidation_0-mlogloss:1.03624\n",
      "[249]\tvalidation_0-mlogloss:1.03621\n",
      "[250]\tvalidation_0-mlogloss:1.03619\n",
      "[251]\tvalidation_0-mlogloss:1.03622\n",
      "[252]\tvalidation_0-mlogloss:1.03628\n",
      "[253]\tvalidation_0-mlogloss:1.03623\n",
      "[254]\tvalidation_0-mlogloss:1.03623\n",
      "[255]\tvalidation_0-mlogloss:1.03622\n",
      "[256]\tvalidation_0-mlogloss:1.03622\n",
      "[257]\tvalidation_0-mlogloss:1.03615\n",
      "[258]\tvalidation_0-mlogloss:1.03608\n",
      "[259]\tvalidation_0-mlogloss:1.03606\n",
      "[260]\tvalidation_0-mlogloss:1.03610\n",
      "[261]\tvalidation_0-mlogloss:1.03614\n",
      "[262]\tvalidation_0-mlogloss:1.03611\n",
      "[263]\tvalidation_0-mlogloss:1.03609\n",
      "[264]\tvalidation_0-mlogloss:1.03607\n",
      "[265]\tvalidation_0-mlogloss:1.03609\n",
      "[266]\tvalidation_0-mlogloss:1.03607\n",
      "[267]\tvalidation_0-mlogloss:1.03609\n",
      "[268]\tvalidation_0-mlogloss:1.03603\n",
      "[269]\tvalidation_0-mlogloss:1.03611\n",
      "[270]\tvalidation_0-mlogloss:1.03609\n",
      "[271]\tvalidation_0-mlogloss:1.03608\n",
      "[272]\tvalidation_0-mlogloss:1.03607\n",
      "[273]\tvalidation_0-mlogloss:1.03613\n",
      "[274]\tvalidation_0-mlogloss:1.03615\n",
      "[275]\tvalidation_0-mlogloss:1.03616\n",
      "[276]\tvalidation_0-mlogloss:1.03619\n",
      "[277]\tvalidation_0-mlogloss:1.03613\n",
      "[278]\tvalidation_0-mlogloss:1.03611\n",
      "[279]\tvalidation_0-mlogloss:1.03604\n",
      "[280]\tvalidation_0-mlogloss:1.03602\n",
      "[281]\tvalidation_0-mlogloss:1.03595\n",
      "[282]\tvalidation_0-mlogloss:1.03594\n",
      "[283]\tvalidation_0-mlogloss:1.03597\n",
      "[284]\tvalidation_0-mlogloss:1.03603\n",
      "[285]\tvalidation_0-mlogloss:1.03602\n",
      "[286]\tvalidation_0-mlogloss:1.03595\n",
      "[287]\tvalidation_0-mlogloss:1.03593\n",
      "[288]\tvalidation_0-mlogloss:1.03592\n",
      "[289]\tvalidation_0-mlogloss:1.03589\n",
      "[290]\tvalidation_0-mlogloss:1.03588\n",
      "[291]\tvalidation_0-mlogloss:1.03590\n",
      "[292]\tvalidation_0-mlogloss:1.03595\n",
      "[293]\tvalidation_0-mlogloss:1.03591\n",
      "[294]\tvalidation_0-mlogloss:1.03584\n",
      "[295]\tvalidation_0-mlogloss:1.03591\n",
      "[296]\tvalidation_0-mlogloss:1.03586\n",
      "[297]\tvalidation_0-mlogloss:1.03579\n",
      "[298]\tvalidation_0-mlogloss:1.03576\n",
      "[299]\tvalidation_0-mlogloss:1.03581\n",
      "[300]\tvalidation_0-mlogloss:1.03576\n",
      "[301]\tvalidation_0-mlogloss:1.03577\n",
      "[302]\tvalidation_0-mlogloss:1.03585\n",
      "[303]\tvalidation_0-mlogloss:1.03588\n",
      "[304]\tvalidation_0-mlogloss:1.03586\n",
      "[305]\tvalidation_0-mlogloss:1.03587\n",
      "[306]\tvalidation_0-mlogloss:1.03588\n",
      "[307]\tvalidation_0-mlogloss:1.03590\n",
      "[308]\tvalidation_0-mlogloss:1.03587\n",
      "[309]\tvalidation_0-mlogloss:1.03584\n",
      "[310]\tvalidation_0-mlogloss:1.03594\n",
      "[311]\tvalidation_0-mlogloss:1.03587\n",
      "[312]\tvalidation_0-mlogloss:1.03585\n",
      "[313]\tvalidation_0-mlogloss:1.03580\n",
      "[314]\tvalidation_0-mlogloss:1.03580\n",
      "[315]\tvalidation_0-mlogloss:1.03578\n",
      "[316]\tvalidation_0-mlogloss:1.03579\n",
      "[317]\tvalidation_0-mlogloss:1.03587\n",
      "[318]\tvalidation_0-mlogloss:1.03583\n",
      "[319]\tvalidation_0-mlogloss:1.03585\n",
      "[320]\tvalidation_0-mlogloss:1.03585\n",
      "[321]\tvalidation_0-mlogloss:1.03589\n",
      "[322]\tvalidation_0-mlogloss:1.03590\n",
      "[323]\tvalidation_0-mlogloss:1.03585\n",
      "[324]\tvalidation_0-mlogloss:1.03582\n",
      "[325]\tvalidation_0-mlogloss:1.03590\n",
      "[326]\tvalidation_0-mlogloss:1.03587\n",
      "[327]\tvalidation_0-mlogloss:1.03588\n",
      "[328]\tvalidation_0-mlogloss:1.03593\n",
      "[329]\tvalidation_0-mlogloss:1.03600\n",
      "[330]\tvalidation_0-mlogloss:1.03606\n",
      "[331]\tvalidation_0-mlogloss:1.03609\n",
      "[332]\tvalidation_0-mlogloss:1.03602\n",
      "[333]\tvalidation_0-mlogloss:1.03603\n",
      "[334]\tvalidation_0-mlogloss:1.03607\n",
      "[335]\tvalidation_0-mlogloss:1.03605\n",
      "[336]\tvalidation_0-mlogloss:1.03606\n",
      "[337]\tvalidation_0-mlogloss:1.03601\n",
      "[338]\tvalidation_0-mlogloss:1.03600\n",
      "[339]\tvalidation_0-mlogloss:1.03606\n",
      "[340]\tvalidation_0-mlogloss:1.03606\n",
      "[341]\tvalidation_0-mlogloss:1.03605\n",
      "[342]\tvalidation_0-mlogloss:1.03607\n",
      "[343]\tvalidation_0-mlogloss:1.03604\n",
      "[344]\tvalidation_0-mlogloss:1.03611\n",
      "[345]\tvalidation_0-mlogloss:1.03609\n",
      "[346]\tvalidation_0-mlogloss:1.03609\n",
      "[347]\tvalidation_0-mlogloss:1.03607\n",
      "[348]\tvalidation_0-mlogloss:1.03611\n",
      "[349]\tvalidation_0-mlogloss:1.03608\n",
      "[350]\tvalidation_0-mlogloss:1.03608\n",
      "[351]\tvalidation_0-mlogloss:1.03605\n",
      "[352]\tvalidation_0-mlogloss:1.03606\n",
      "[353]\tvalidation_0-mlogloss:1.03609\n",
      "[354]\tvalidation_0-mlogloss:1.03600\n",
      "[355]\tvalidation_0-mlogloss:1.03604\n",
      "[356]\tvalidation_0-mlogloss:1.03603\n",
      "[357]\tvalidation_0-mlogloss:1.03601\n",
      "[358]\tvalidation_0-mlogloss:1.03602\n",
      "[359]\tvalidation_0-mlogloss:1.03600\n",
      "[360]\tvalidation_0-mlogloss:1.03598\n",
      "[361]\tvalidation_0-mlogloss:1.03592\n",
      "[362]\tvalidation_0-mlogloss:1.03597\n",
      "[363]\tvalidation_0-mlogloss:1.03602\n",
      "[364]\tvalidation_0-mlogloss:1.03605\n",
      "[365]\tvalidation_0-mlogloss:1.03608\n",
      "[366]\tvalidation_0-mlogloss:1.03607\n",
      "[367]\tvalidation_0-mlogloss:1.03602\n",
      "[368]\tvalidation_0-mlogloss:1.03610\n",
      "[369]\tvalidation_0-mlogloss:1.03607\n",
      "[370]\tvalidation_0-mlogloss:1.03607\n",
      "[371]\tvalidation_0-mlogloss:1.03604\n",
      "[372]\tvalidation_0-mlogloss:1.03601\n",
      "[373]\tvalidation_0-mlogloss:1.03605\n",
      "[374]\tvalidation_0-mlogloss:1.03610\n",
      "[375]\tvalidation_0-mlogloss:1.03603\n",
      "[376]\tvalidation_0-mlogloss:1.03607\n",
      "[377]\tvalidation_0-mlogloss:1.03611\n",
      "[378]\tvalidation_0-mlogloss:1.03602\n",
      "[379]\tvalidation_0-mlogloss:1.03602\n",
      "[380]\tvalidation_0-mlogloss:1.03609\n",
      "[381]\tvalidation_0-mlogloss:1.03608\n",
      "[382]\tvalidation_0-mlogloss:1.03611\n",
      "[383]\tvalidation_0-mlogloss:1.03615\n",
      "[384]\tvalidation_0-mlogloss:1.03611\n",
      "[385]\tvalidation_0-mlogloss:1.03621\n",
      "[386]\tvalidation_0-mlogloss:1.03624\n",
      "[387]\tvalidation_0-mlogloss:1.03630\n",
      "[388]\tvalidation_0-mlogloss:1.03627\n",
      "[389]\tvalidation_0-mlogloss:1.03626\n",
      "[390]\tvalidation_0-mlogloss:1.03627\n",
      "[391]\tvalidation_0-mlogloss:1.03633\n",
      "[392]\tvalidation_0-mlogloss:1.03638\n",
      "[393]\tvalidation_0-mlogloss:1.03638\n",
      "[394]\tvalidation_0-mlogloss:1.03634\n",
      "[395]\tvalidation_0-mlogloss:1.03631\n",
      "[396]\tvalidation_0-mlogloss:1.03632\n",
      "[397]\tvalidation_0-mlogloss:1.03629\n",
      "[398]\tvalidation_0-mlogloss:1.03625\n",
      "[399]\tvalidation_0-mlogloss:1.03619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.conda/envs/jun_cls/lib/python3.10/site-packages/xgboost/core.py:705: UserWarning: [18:58:57] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1758008603490/work/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- XGBOOST Validation LogLoss: 1.035757 ---\n",
      "Training complete. Time taken: 120.73s\n",
      "Model saved to: ./models/candidate_2_xgboost_e5-base-v2.pkl\n"
     ]
    }
   ],
   "source": [
    "# Path to save the trained GBM model\n",
    "GBM_CHOICE = \"XGBOOST\" \n",
    "CANDIDATE_2_MODEL_SAVE_PATH = f\"./models/candidate_2_{GBM_CHOICE.lower()}_{BEST_EMBEDDING_MODEL_PATH.split('/')[-1]}.pkl\"\n",
    "print(f\"--- Candidate 2: {GBM_CHOICE} ---\")\n",
    "print(f\"Model will be saved to: {CANDIDATE_2_MODEL_SAVE_PATH}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# This dictionary will hold the best validation logloss\n",
    "val_logloss = {}\n",
    "clf_c2 = None\n",
    "\n",
    "if GBM_CHOICE == \"LGBM\":\n",
    "    # --- 2.1. LightGBM ---\n",
    "    clf_c2 = lgb.LGBMClassifier(\n",
    "        objective='multiclass',\n",
    "        metric='multi_logloss',\n",
    "        num_class=3,\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state,\n",
    "        # device='gpu' if device == 'cuda' else 'cpu'\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    clf_c2.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        eval_metric='multi_logloss',\n",
    "        callbacks=[lgb.early_stopping(100, verbose=True)]\n",
    "    )\n",
    "    \n",
    "    va_pred = clf_c2.predict_proba(X_va)\n",
    "    val_logloss['LGBM'] = log_loss(y_va, va_pred)\n",
    "    print(f\"--- LGBM Validation LogLoss: {val_logloss['LGBM']:.6f} ---\")\n",
    "\n",
    "elif GBM_CHOICE == \"XGBOOST\":\n",
    "    # --- 2.2. XGBoost ---\n",
    "    clf_c2 = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        eval_metric='mlogloss',\n",
    "        num_class=3,\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        n_jobs=-1,\n",
    "        random_state=random_state,\n",
    "        device=device,\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "    \n",
    "    clf_c2.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    va_pred = clf_c2.predict_proba(X_va)\n",
    "    val_logloss['XGBOOST'] = log_loss(y_va, va_pred)\n",
    "    print(f\"--- XGBOOST Validation LogLoss: {val_logloss['XGBOOST']:.6f} ---\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error: Unknown GBM_CHOICE '{GBM_CHOICE}'. Please set to 'LGBM' or 'XGBOOST'.\")\n",
    "\n",
    "print(f\"Training complete. Time taken: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "if clf_c2 is not None:\n",
    "    # Use joblib for cross-compatibility between LGBM/XGB\n",
    "    joblib.dump(clf_c2, CANDIDATE_2_MODEL_SAVE_PATH)\n",
    "    print(f\"Model saved to: {CANDIDATE_2_MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc172072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "print(f\"\\n=== Candidate 2 ({GBM_CHOICE}): Calibration ===\")\n",
    "\n",
    "# Get validation predictions BEFORE calibration\n",
    "# We need to recreate the train/val split from earlier\n",
    "X_tr_c2, X_va_c2, y_tr_c2, y_va_c2 = train_test_split(\n",
    "    X_c2, y, test_size=val_size, stratify=y, random_state=random_state\n",
    ")\n",
    "\n",
    "# Get predictions from the validation model (clf_c2 trained on X_tr, X_va split)\n",
    "if 'clf_c2' in locals() and clf_c2 is not None:\n",
    "    va_pred_before = clf_c2.predict_proba(X_va_c2)\n",
    "    logloss_before = log_loss(y_va_c2, va_pred_before)\n",
    "    print(f\"Validation LogLoss BEFORE Calibration: {logloss_before:.6f}\")\n",
    "    \n",
    "    # Apply calibration (using isotonic regression)\n",
    "    print(\"Applying Isotonic calibration...\")\n",
    "    calibrated_model_c2 = CalibratedClassifierCV(\n",
    "        clf_c2,\n",
    "        method='isotonic',  # isotonic or sigmoid\n",
    "        cv='prefit',  # Model is already fitted\n",
    "        ensemble=False\n",
    "    )\n",
    "    \n",
    "    # Fit calibration on validation set\n",
    "    calibrated_model_c2.fit(X_va_c2, y_va_c2)\n",
    "    \n",
    "    # Get calibrated predictions on validation set\n",
    "    va_pred_after = calibrated_model_c2.predict_proba(X_va_c2)\n",
    "    logloss_after = log_loss(y_va_c2, va_pred_after)\n",
    "    print(f\"Validation LogLoss AFTER Calibration: {logloss_after:.6f}\")\n",
    "    print(f\"Improvement: {logloss_before - logloss_after:.6f}\")\n",
    "    \n",
    "    # Now retrain on full data and apply calibration\n",
    "    print(\"\\nRetraining on full data for final submission...\")\n",
    "    \n",
    "    # Get best iteration\n",
    "    try:\n",
    "        best_iter = clf_c2.best_iteration_ or 1000\n",
    "    except AttributeError:\n",
    "        best_iter = 1000\n",
    "    \n",
    "    X_train_full, X_cal, y_train_full, y_cal = train_test_split(\n",
    "        X_c2, y, test_size=val_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    if GBM_CHOICE == \"LGBM\":\n",
    "        clf_c2_for_calib = lgb.LGBMClassifier(\n",
    "            objective='multiclass', metric='multi_logloss', num_class=3,\n",
    "            n_estimators=best_iter, learning_rate=0.05,\n",
    "            n_jobs=-1, random_state=random_state, device=device\n",
    "        )\n",
    "    elif GBM_CHOICE == \"XGBOOST\":\n",
    "        clf_c2_for_calib = xgb.XGBClassifier(\n",
    "            objective='multi:softprob', eval_metric='mlogloss', num_class=3,\n",
    "            n_estimators=best_iter, learning_rate=0.05,\n",
    "            n_jobs=-1, random_state=random_state, device=device\n",
    "        )\n",
    "    \n",
    "    clf_c2_for_calib.fit(X_train_full, y_train_full)\n",
    "    \n",
    "    # Apply calibration on the hold-out 20%\n",
    "    calibrated_final_c2 = CalibratedClassifierCV(\n",
    "        clf_c2_for_calib,\n",
    "        method='isotonic',\n",
    "        cv='prefit',\n",
    "        ensemble=False\n",
    "    )\n",
    "    calibrated_final_c2.fit(X_cal, y_cal)\n",
    "    \n",
    "    C2_CALIBRATED_MODEL_PATH = f\"./models/candidate_2_{GBM_CHOICE}_{MODEL_NAME}_CALIBRATED.pkl\"\n",
    "    joblib.dump(calibrated_final_c2, C2_CALIBRATED_MODEL_PATH)\n",
    "    print(f\"Candidate 2 Calibrated Model SAVED to {C2_CALIBRATED_MODEL_PATH}\")\n",
    "    \n",
    "    print(f\"\\n=== Candidate 2 Summary ===\")\n",
    "    print(f\"Before Calibration - Val LogLoss: {logloss_before:.6f}\")\n",
    "    print(f\"After Calibration  - Val LogLoss: {logloss_after:.6f}\")\n",
    "    print(f\"Final submission saved with calibration.\")\n",
    "else:\n",
    "    print(\"Error: clf_c2 model not found. Please run the training cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ae5691",
   "metadata": {},
   "source": [
    "### Candidate 3: All Features + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "160ce994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Candidate 3: All Features + MLP ---\n",
      "Building strong lexical features...\n",
      "Strong lexical features shape: (57477, 30)\n"
     ]
    }
   ],
   "source": [
    "### Candidate 3: All Features + MLP ###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define the \"Strong\" lexical feature builder\n",
    "def stats_strong(s):\n",
    "    \"\"\"Calculates a comprehensive set of lexical statistics.\"\"\"\n",
    "    if not isinstance(s, str): s = \"\"\n",
    "    toks = s.split()\n",
    "    return {\n",
    "        \"len_char\": len(s),\n",
    "        \"len_tok\": len(toks),\n",
    "        \"num_sent\": sum(s.count(x) for x in [\".\", \"!\", \"?\"]),\n",
    "        \"num_code\": s.count(\"`\"),\n",
    "        \"num_list\": s.count(\"- \") + s.count(\"* \"),\n",
    "        \"num_upper\": sum(ch.isupper() for ch in s),\n",
    "        \"num_punct\": sum(ch in \",;:()\" for ch in s),\n",
    "        \"avg_tok_len\": (sum(len(t) for t in toks) / len(toks)) if toks else 0.0,\n",
    "    }\n",
    "\n",
    "def build_strong_lexical_features(df):\n",
    "    \"\"\"Builds the full set of lexical and bias features.\"\"\"\n",
    "    rows = []\n",
    "    cols = [\"prompt\", \"response_a\", \"response_b\"]\n",
    "    \n",
    "    for p, a, b in zip(df[cols[0]], df[cols[1]], df[cols[2]]):\n",
    "        ps, as_, bs = stats_strong(p), stats_strong(a), stats_strong(b)\n",
    "        rows.append({\n",
    "            \"p_len_char\": ps[\"len_char\"], \"p_len_tok\": ps[\"len_tok\"], \"p_num_sent\": ps[\"num_sent\"],\n",
    "            \"a_len_char\": as_[\"len_char\"], \"a_len_tok\": as_[\"len_tok\"], \"a_num_sent\": as_[\"num_sent\"],\n",
    "            \"a_num_code\": as_[\"num_code\"], \"a_num_list\": as_[\"num_list\"], \"a_num_upper\": as_[\"num_upper\"],\n",
    "            \"a_num_punct\": as_[\"num_punct\"], \"a_avg_tok_len\": as_[\"avg_tok_len\"],\n",
    "            \"b_len_char\": bs[\"len_char\"], \"b_len_tok\": bs[\"len_tok\"], \"b_num_sent\": bs[\"num_sent\"],\n",
    "            \"b_num_code\": bs[\"num_code\"], \"b_num_list\": bs[\"num_list\"], \"b_num_upper\": bs[\"num_upper\"],\n",
    "            \"b_num_punct\": bs[\"num_punct\"], \"b_avg_tok_len\": bs[\"avg_tok_len\"],\n",
    "            # A-B Differences\n",
    "            \"d_len_char\": as_[\"len_char\"] - bs[\"len_char\"],\n",
    "            \"d_len_tok\": as_[\"len_tok\"] - bs[\"len_tok\"],\n",
    "            \"d_num_sent\": as_[\"num_sent\"] - bs[\"num_sent\"],\n",
    "            \"d_num_code\": as_[\"num_code\"] - bs[\"num_code\"],\n",
    "            \"d_num_list\": as_[\"num_list\"] - bs[\"num_list\"],\n",
    "            \"d_num_upper\": as_[\"num_upper\"] - bs[\"num_upper\"],\n",
    "            \"d_num_punct\": as_[\"num_punct\"] - bs[\"num_punct\"],\n",
    "            \"d_avg_tok_len\": as_[\"avg_tok_len\"] - bs[\"avg_tok_len\"],\n",
    "            # Ratios\n",
    "            \"r_len_char\": (as_[\"len_char\"] + 1) / (bs[\"len_char\"] + 1),\n",
    "            \"r_len_tok\": (as_[\"len_tok\"] + 1) / (bs[\"len_tok\"] + 1),\n",
    "            \"r_num_sent\": (as_[\"num_sent\"] + 1) / (bs[\"num_sent\"] + 1),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print(\"--- Candidate 3: All Features + MLP ---\")\n",
    "print(\"Building strong lexical features...\")\n",
    "\n",
    "# Generate Lexical Features\n",
    "X_lex_strong = build_strong_lexical_features(train)\n",
    "X_test_lex_strong = build_strong_lexical_features(test)\n",
    "\n",
    "print(f\"Strong lexical features shape: {X_lex_strong.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d71772e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using embedding model: ./models/e5-base-v2\n",
      "try: ./models/e5-base-v2\n",
      "loaded model from: ./models/e5-base-v2\n",
      "Encoding texts\n",
      "Prompt encoding complete.\n",
      "Response A encoding complete.\n",
      "Response B encoding complete.\n",
      "Encoding test data...\n",
      "Encoding complete for test data.\n",
      "Feature extraction complete. Time taken: 1461.08s\n",
      "Train features shape (X_c3): (57477, 6144)\n",
      "Test features shape (X_test_c3): (3, 6144)\n",
      "Data split into: Train (45981, 6144), Validation (11496, 6144)\n"
     ]
    }
   ],
   "source": [
    "import joblib # For saving models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"e5-base-v2\" \n",
    "BEST_EMBEDDING_MODEL_PATH = f\"./models/{MODEL_NAME}\"\n",
    "print(f\"Using embedding model: {BEST_EMBEDDING_MODEL_PATH}\")\n",
    "\n",
    "# Load the chosen embedding model\n",
    "try:\n",
    "    # We pass a list containing only our chosen model path\n",
    "    sbert_model, model_src = load_model([BEST_EMBEDDING_MODEL_PATH], idx=0, device=device)\n",
    "    # print(f\"Successfully loaded model from: {model_src}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model from {BEST_EMBEDDING_MODEL_PATH}. Error: {e}\")\n",
    "\n",
    "print(\"Encoding texts\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Encode training data\n",
    "prompt_emb = encode_texts(sbert_model, train[\"prompt\"])\n",
    "print(\"Prompt encoding complete.\")\n",
    "a_emb = encode_texts(sbert_model, train[\"response_a\"])\n",
    "print(\"Response A encoding complete.\")\n",
    "b_emb = encode_texts(sbert_model, train[\"response_b\"])\n",
    "print(\"Response B encoding complete.\")\n",
    "\n",
    "# Build training features\n",
    "X_c3 = build_feat(prompt_emb, a_emb, b_emb) # X for candidate 3\n",
    "\n",
    "# Encode test data\n",
    "print(\"Encoding test data...\")\n",
    "prompt_emb_te = encode_texts(sbert_model, test[\"prompt\"])\n",
    "a_emb_te = encode_texts(sbert_model, test[\"response_a\"])\n",
    "b_emb_te = encode_texts(sbert_model, test[\"response_b\"])\n",
    "print(\"Encoding complete for test data.\")\n",
    "\n",
    "# Build test features\n",
    "X_test_c3 = build_feat(prompt_emb_te, a_emb_te, b_emb_te)\n",
    "\n",
    "# Clean up model from memory\n",
    "del sbert_model, prompt_emb, a_emb, b_emb, prompt_emb_te, a_emb_te, b_emb_te\n",
    "\n",
    "print(f\"Feature extraction complete. Time taken: {time.time() - start_time:.2f}s\")\n",
    "print(f\"Train features shape (X_c3): {X_c3.shape}\")\n",
    "print(f\"Test features shape (X_test_c3): {X_test_c3.shape}\")\n",
    "\n",
    "# Create Train/Validation Split\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X_c3, y, test_size=val_size, stratify=y)\n",
    "print(f\"Data split into: Train {X_tr.shape}, Validation {X_va.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3c98530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining lexical and embedding features...\n",
      "Combined train features shape (X_c3): (57477, 6174)\n",
      "Combined test features shape (X_test_c3): (3, 6174)\n",
      "Scaling features...\n",
      "Training MLPClassifier...\n",
      "Iteration 1, loss = 1.12600698\n",
      "Validation score: 0.463796\n",
      "Iteration 2, loss = 0.82086238\n",
      "Validation score: 0.447271\n",
      "Iteration 3, loss = 0.53996426\n",
      "Validation score: 0.430093\n",
      "Iteration 4, loss = 0.26805010\n",
      "Validation score: 0.429441\n",
      "Iteration 5, loss = 0.12704382\n",
      "Validation score: 0.448141\n",
      "Iteration 6, loss = 0.06509657\n",
      "Validation score: 0.437486\n",
      "Iteration 7, loss = 0.04185474\n",
      "Validation score: 0.436834\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Training complete. Time taken: 56.44s\n",
      "--- MLP (All Features) Validation LogLoss: 1.048909 ---\n",
      "MLP model saved to: ./models/candidate_3_mlp.pkl\n",
      "Scaler saved to: ./models/candidate_3_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Handle NaNs/Infs for safety\n",
    "X_lex_strong = X_lex_strong.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "X_test_lex_strong = X_test_lex_strong.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# Convert embedding features (numpy) to DataFrame for safe processing\n",
    "X_c3_safe = pd.DataFrame(X_c3).fillna(0).replace([np.inf, -np.inf], 0).values\n",
    "X_test_c3_safe = pd.DataFrame(X_test_c3).fillna(0).replace([np.inf, -np.inf], 0).values\n",
    "\n",
    "print(\"Combining lexical and embedding features...\")\n",
    "X_c3 = np.hstack([X_lex_strong, X_c3_safe])\n",
    "X_test_c3 = np.hstack([X_test_lex_strong, X_test_c3_safe])\n",
    "print(f\"Combined train features shape (X_c3): {X_c3.shape}\")\n",
    "print(f\"Combined test features shape (X_test_c3): {X_test_c3.shape}\")\n",
    "\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X_c3, y, test_size=val_size, stratify=y, random_state=random_state)\n",
    "\n",
    "print(\"Scaling features...\")\n",
    "scaler_c3 = StandardScaler()\n",
    "X_tr_sc = scaler_c3.fit_transform(X_tr)\n",
    "X_va_sc = scaler_c3.transform(X_va)\n",
    "\n",
    "# Train MLP Classifier\n",
    "print(\"Training MLPClassifier...\")\n",
    "start_time = time.time()\n",
    "\n",
    "clf_c3 = MLPClassifier(\n",
    "    hidden_layer_sizes=(512, 256),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    alpha=1e-4,          # L2 regularization\n",
    "    batch_size=512,\n",
    "    learning_rate_init=1e-3,\n",
    "    max_iter=100,        \n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=5,\n",
    "    random_state=random_state,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "clf_c3.fit(X_tr_sc, y_tr)\n",
    "\n",
    "va_pred = clf_c3.predict_proba(X_va_sc)\n",
    "val_logloss = log_loss(y_va, va_pred)\n",
    "\n",
    "print(f\"Training complete. Time taken: {time.time() - start_time:.2f}s\")\n",
    "print(f\"--- MLP (All Features) Validation LogLoss: {val_logloss:.6f} ---\")\n",
    "\n",
    "CANDIDATE_3_MODEL_SAVE_PATH = \"./models/candidate_3_mlp.pkl\"\n",
    "CANDIDATE_3_SCALER_SAVE_PATH = \"./models/candidate_3_scaler.pkl\"\n",
    "\n",
    "joblib.dump(clf_c3, CANDIDATE_3_MODEL_SAVE_PATH)\n",
    "joblib.dump(scaler_c3, CANDIDATE_3_SCALER_SAVE_PATH)\n",
    "\n",
    "print(f\"MLP model saved to: {CANDIDATE_3_MODEL_SAVE_PATH}\")\n",
    "print(f\"Scaler saved to: {CANDIDATE_3_SCALER_SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797e9f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Candidate 3 (MLP All Features): Calibration ===\n",
      "Validation LogLoss BEFORE Calibration: 1.048909\n",
      "Applying Isotonic calibration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.conda/envs/jun_cls/lib/python3.10/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation LogLoss AFTER Calibration: 1.036883\n",
      "Improvement: 0.012026\n",
      "\n",
      "Retraining on full data for final submission...\n",
      "Creating submission file: submission_candidate3_MLP_AllFeatures_CALIBRATED.csv...\n",
      "Successfully saved and verified: submission_candidate3_MLP_AllFeatures_CALIBRATED.csv (Shape: (3, 4))\n",
      "\n",
      "=== Candidate 3 Summary ===\n",
      "Before Calibration - Val LogLoss: 1.048909\n",
      "After Calibration  - Val LogLoss: 1.036883\n",
      "Final submission saved with calibration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.conda/envs/jun_cls/lib/python3.10/site-packages/sklearn/calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n=== Candidate 3 (MLP All Features): Calibration ===\")\n",
    "\n",
    "# Get validation predictions BEFORE calibration\n",
    "# We need to recreate the train/val split from earlier\n",
    "X_tr_c3, X_va_c3, y_tr_c3, y_va_c3 = train_test_split(\n",
    "    X_c3, y, test_size=val_size, stratify=y, random_state=random_state\n",
    ")\n",
    "\n",
    "# Scale validation data\n",
    "scaler_c3_val = StandardScaler()\n",
    "X_tr_sc_c3 = scaler_c3_val.fit_transform(X_tr_c3)\n",
    "X_va_sc_c3 = scaler_c3_val.transform(X_va_c3)\n",
    "\n",
    "# Get predictions from the validation model (clf_c3 trained on X_tr, X_va split)\n",
    "if 'clf_c3' in locals() and clf_c3 is not None:\n",
    "    va_pred_before = clf_c3.predict_proba(X_va_sc_c3)\n",
    "    logloss_before = log_loss(y_va_c3, va_pred_before)\n",
    "    print(f\"Validation LogLoss BEFORE Calibration: {logloss_before:.6f}\")\n",
    "    \n",
    "    # Apply calibration (using isotonic regression)\n",
    "    print(\"Applying Isotonic calibration...\")\n",
    "    calibrated_model_c3 = CalibratedClassifierCV(\n",
    "        clf_c3,\n",
    "        method='isotonic',  # isotonic or sigmoid\n",
    "        cv='prefit',  # Model is already fitted\n",
    "        ensemble=False\n",
    "    )\n",
    "    \n",
    "    # Fit calibration on validation set\n",
    "    calibrated_model_c3.fit(X_va_sc_c3, y_va_c3)\n",
    "    \n",
    "    # Get calibrated predictions on validation set\n",
    "    va_pred_after = calibrated_model_c3.predict_proba(X_va_sc_c3)\n",
    "    logloss_after = log_loss(y_va_c3, va_pred_after)\n",
    "    print(f\"Validation LogLoss AFTER Calibration: {logloss_after:.6f}\")\n",
    "    print(f\"Improvement: {logloss_before - logloss_after:.6f}\")\n",
    "    \n",
    "    # Now retrain on full data and apply calibration\n",
    "    print(\"\\nRetraining on full data for final submission...\")\n",
    "    0\n",
    "    X_train_full, X_cal, y_train_full, y_cal = train_test_split(\n",
    "        X_c3, y, test_size=val_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Scale\n",
    "    scaler_final_c3 = StandardScaler()\n",
    "    X_train_full_sc = scaler_final_c3.fit_transform(X_train_full)\n",
    "    X_cal_sc = scaler_final_c3.transform(X_cal)\n",
    "    X_test_final_sc = scaler_final_c3.transform(X_test_c3)\n",
    "    \n",
    "    # Train MLP\n",
    "    clf_c3_for_calib = MLPClassifier(\n",
    "        hidden_layer_sizes=(512, 256), activation=\"relu\", solver=\"adam\",\n",
    "        alpha=1e-4, batch_size=512, learning_rate_init=1e-3,\n",
    "        max_iter=100, early_stopping=True, n_iter_no_change=5,\n",
    "        random_state=random_state, verbose=False\n",
    "    )\n",
    "    clf_c3_for_calib.fit(X_train_full_sc, y_train_full)\n",
    "    \n",
    "    # Apply calibration on the hold-out 20%\n",
    "    calibrated_final_c3 = CalibratedClassifierCV(\n",
    "        clf_c3_for_calib,\n",
    "        method='isotonic',\n",
    "        cv='prefit',\n",
    "        ensemble=False\n",
    "    )\n",
    "    calibrated_final_c3.fit(X_cal_sc, y_cal)\n",
    "    \n",
    "    C3_CALIBRATED_MODEL_PATH = \"./models/candidate_3_MLP_CALIBRATED.pkl\"\n",
    "    C3_FINAL_SCALER_PATH = \"./models/candidate_3_scaler_final.pkl\"\n",
    "    joblib.dump(calibrated_final_c3, C3_CALIBRATED_MODEL_PATH)\n",
    "    joblib.dump(scaler_final_c3, C3_FINAL_SCALER_PATH)\n",
    "    print(f\"Candidate 3 Calibrated Model SAVED to {C3_CALIBRATED_MODEL_PATH}\")\n",
    "    print(f\"Candidate 3 Final Scaler SAVED to {C3_FINAL_SCALER_PATH}\")\n",
    "    \n",
    "    print(f\"\\n=== Candidate 3 Summary ===\")\n",
    "    print(f\"Before Calibration - Val LogLoss: {logloss_before:.6f}\")\n",
    "    print(f\"After Calibration  - Val LogLoss: {logloss_after:.6f}\")\n",
    "    print(f\"Final submission saved with calibration.\")\n",
    "else:\n",
    "    print(\"Error: clf_c3 model not found. Please run the training cell first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jun_cls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
