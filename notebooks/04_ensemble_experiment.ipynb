{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f1af539",
   "metadata": {},
   "source": [
    "# Team7 Assignment 2: Kaggle Inference Notebook (LLM Classification Finetuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e388819",
   "metadata": {},
   "source": [
    "### Setting and Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d53d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "data_load",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.conda/envs/jun_cls2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: ../datasets (57477, 9) (3, 4)\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "import torch\n",
    "import joblib, time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import softmax\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import glob\n",
    "\n",
    "# =========================================================================\n",
    "# Need to set path for Kaggle Dataset\n",
    "# =========================================================================\n",
    "KAGGLE_MODEL_DIR = \"../models\"\n",
    "KAGGLE_DATA_DIR = \"../datasets\"\n",
    "BASE_DIR = KAGGLE_MODEL_DIR\n",
    "\n",
    "# Data Load\n",
    "train = pd.read_csv(f\"{KAGGLE_DATA_DIR}/train.csv\")\n",
    "test  = pd.read_csv(f\"{KAGGLE_DATA_DIR}/test.csv\")\n",
    "sample = pd.read_csv(f\"{KAGGLE_DATA_DIR}/sample_submission.csv\")\n",
    "\n",
    "print(\"DATA:\", KAGGLE_DATA_DIR, train.shape, test.shape)\n",
    "\n",
    "# 0: model_a win, 1: model_b win, 2: tie\n",
    "y = train[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values.argmax(1)\n",
    "\n",
    "random_state = 20010815\n",
    "val_size = 0.2\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "global_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All functions loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Global Functions ###\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(random_state)\n",
    "random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_state)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def load_model(candidates, idx=0, device=\"cpu\"):\n",
    "    # Load\n",
    "    last_err = None\n",
    "    path = candidates[idx]\n",
    "    try:\n",
    "        print(\"try:\", path)\n",
    "        model = SentenceTransformer(path, device=device)\n",
    "        print(\"loaded model from:\", path)\n",
    "        return model, path\n",
    "    except Exception as e:\n",
    "        last_err = e\n",
    "    raise RuntimeError(\"Failed to load model. In Kaggle, you need to upload the model folder to Datasets and then link it via 'Add data'. Last error: \" + str(last_err))\n",
    "\n",
    "\n",
    "def build_feat(P, A, B):\n",
    "    AB_diff = A - B\n",
    "    AB_adiff = np.abs(AB_diff)\n",
    "    AB_mul = A * B\n",
    "    PA_mul = P * A\n",
    "    PB_mul = P * B\n",
    "    return np.hstack([P, A, B, AB_diff, AB_adiff, AB_mul, PA_mul, PB_mul])\n",
    "\n",
    "\n",
    "def l2norm(a, eps=1e-12):\n",
    "    n = np.linalg.norm(a, axis=1, keepdims=True)\n",
    "    n = np.clip(n, eps, None)\n",
    "    return a / n\n",
    "\n",
    "def encode_texts(model, texts, batch_size=256):\n",
    "    vecs = []\n",
    "    total_texts = len(texts)\n",
    "    total_batches = (total_texts + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        start_time = time.time()\n",
    "        batch = texts[i:i+batch_size].tolist() if isinstance(texts, pd.Series) else texts[i:i+batch_size]\n",
    "        v = model.encode(batch, batch_size=len(batch), convert_to_numpy=True, normalize_embeddings=False, show_progress_bar=False)\n",
    "        vecs.append(v)\n",
    "\n",
    "        batch_num = (i // batch_size) + 1\n",
    "        print(f\"{batch_num}/{total_batches} | time: {time.time() - start_time:.2f}s\", end='\\r', flush=True)\n",
    "    V = np.vstack(vecs)\n",
    "    return l2norm(V)\n",
    "\n",
    "\n",
    "def create_and_save_submission(predictions, filename, test_df, sample_df):\n",
    "    \"\"\"\n",
    "    Creates a Kaggle submission file from model predictions.\n",
    "    Then, it normalizes the probabilities, performs validation checks, and saves the file.\n",
    "    Args:\n",
    "        predictions (np.array): return value of predict_proba() (N, 3)\n",
    "        filename (str): csv filename to save the submission.\n",
    "        test_df (pd.DataFrame): dataframe containing 'id' column.\n",
    "        sample_df (pd.DataFrame): dataframe to align columns with sample submission.\n",
    "    \"\"\"\n",
    "    print(f\"Creating submission file: {filename}...\")\n",
    "    \n",
    "    # 1. Save Submission File\n",
    "    sub_df = pd.DataFrame({\n",
    "        \"id\": test_df[\"id\"],\n",
    "        \"winner_model_a\": predictions[:, 0],\n",
    "        \"winner_model_b\": predictions[:, 1],\n",
    "        \"winner_tie\":     predictions[:, 2],\n",
    "    })\n",
    "\n",
    "    # 2. Normalization check (safety)\n",
    "    probs = sub_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "    row_sums = probs.sum(axis=1, keepdims=True)\n",
    "    probs = probs / np.clip(row_sums, 1e-15, None)\n",
    "    sub_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]] = probs\n",
    "\n",
    "    # 3. Align columns with sample submission\n",
    "    try:\n",
    "        sub_df = sub_df[sample_df.columns]\n",
    "    except KeyError as e:\n",
    "        print(f\"Warning: Columns in sample_df not found. Saving with default columns. Error: {e}\")\n",
    "\n",
    "    # 4. Save\n",
    "    sub_df.to_csv(filename, index=False)\n",
    "\n",
    "    # 5. Assertions to check file integrity\n",
    "    try:\n",
    "        chk = pd.read_csv(filename)\n",
    "        \n",
    "        assert list(chk.columns) == list(sample_df.columns), \\\n",
    "            f\"Column mismatch. Expected: {list(sample_df.columns)}, Got: {list(chk.columns)}\"\n",
    "        \n",
    "        assert not chk.isna().any().any(), \"NaN values found in submission file.\"\n",
    "        \n",
    "        prob_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "        assert np.allclose(chk[prob_cols].sum(1).values, 1.0), \\\n",
    "            \"Probabilities do not sum to 1.0 for all rows.\"\n",
    "            \n",
    "        print(f\"Successfully saved and verified: {filename} (Shape: {sub_df.shape})\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found after saving: {filename}\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Error: Submission file verification failed! {e}\")\n",
    "    \n",
    "    return sub_df\n",
    "\n",
    "def build_strong_lexical_features(df):\n",
    "    \"\"\"Builds the full set of lexical and bias features.\"\"\"\n",
    "    rows = []\n",
    "    cols = [\"prompt\", \"response_a\", \"response_b\"]\n",
    "    \n",
    "    for p, a, b in zip(df[cols[0]], df[cols[1]], df[cols[2]]):\n",
    "        ps, as_, bs = stats_strong(p), stats_strong(a), stats_strong(b)\n",
    "        rows.append({\n",
    "            \"p_len_char\": ps[\"len_char\"], \"p_len_tok\": ps[\"len_tok\"], \"p_num_sent\": ps[\"num_sent\"],\n",
    "            \"a_len_char\": as_[\"len_char\"], \"a_len_tok\": as_[\"len_tok\"], \"a_num_sent\": as_[\"num_sent\"],\n",
    "            \"a_num_code\": as_[\"num_code\"], \"a_num_list\": as_[\"num_list\"], \"a_num_upper\": as_[\"num_upper\"],\n",
    "            \"a_num_punct\": as_[\"num_punct\"], \"a_avg_tok_len\": as_[\"avg_tok_len\"],\n",
    "            \"b_len_char\": bs[\"len_char\"], \"b_len_tok\": bs[\"len_tok\"], \"b_num_sent\": bs[\"num_sent\"],\n",
    "            \"b_num_code\": bs[\"num_code\"], \"b_num_list\": bs[\"num_list\"], \"b_num_upper\": bs[\"num_upper\"],\n",
    "            \"b_num_punct\": bs[\"num_punct\"], \"b_avg_tok_len\": bs[\"avg_tok_len\"],\n",
    "            # A-B Differences\n",
    "            \"d_len_char\": as_[\"len_char\"] - bs[\"len_char\"],\n",
    "            \"d_len_tok\": as_[\"len_tok\"] - bs[\"len_tok\"],\n",
    "            \"d_num_sent\": as_[\"num_sent\"] - bs[\"num_sent\"],\n",
    "            \"d_num_code\": as_[\"num_code\"] - bs[\"num_code\"],\n",
    "            \"d_num_list\": as_[\"num_list\"] - bs[\"num_list\"],\n",
    "            \"d_num_upper\": as_[\"num_upper\"] - bs[\"num_upper\"],\n",
    "            \"d_num_punct\": as_[\"num_punct\"] - bs[\"num_punct\"],\n",
    "            \"d_avg_tok_len\": as_[\"avg_tok_len\"] - bs[\"avg_tok_len\"],\n",
    "            # Ratios\n",
    "            \"r_len_char\": (as_[\"len_char\"] + 1) / (bs[\"len_char\"] + 1),\n",
    "            \"r_len_tok\": (as_[\"len_tok\"] + 1) / (bs[\"len_tok\"] + 1),\n",
    "            \"r_num_sent\": (as_[\"num_sent\"] + 1) / (bs[\"num_sent\"] + 1),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Define the \"Strong\" lexical feature builder\n",
    "def stats_strong(s):\n",
    "    \"\"\"Calculates a comprehensive set of lexical statistics.\"\"\"\n",
    "    if not isinstance(s, str): s = \"\"\n",
    "    toks = s.split()\n",
    "    return {\n",
    "        \"len_char\": len(s),\n",
    "        \"len_tok\": len(toks),\n",
    "        \"num_sent\": sum(s.count(x) for x in [\".\", \"!\", \"?\"]),\n",
    "        \"num_code\": s.count(\"`\"),\n",
    "        \"num_list\": s.count(\"- \") + s.count(\"* \"),\n",
    "        \"num_upper\": sum(ch.isupper() for ch in s),\n",
    "        \"num_punct\": sum(ch in \",;:()\" for ch in s),\n",
    "        \"avg_tok_len\": (sum(len(t) for t in toks) / len(toks)) if toks else 0.0,\n",
    "    }\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # This formats the input as: [CLS] prompt [SEP] A: response_a [SEP] B: response_b [SEP]\n",
    "    # This is a robust way to present the three pieces of text to the model\n",
    "    \n",
    "    # Combine response_a and response_b into a single string\n",
    "    response_pair = [f\"A: {a} {tokenizer.sep_token} B: {b}\" for a, b in zip(examples['response_a'], examples['response_b'])]\n",
    "    \n",
    "    # Tokenize, using prompt as the first sequence and the combined response as the second\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['prompt'],\n",
    "        response_pair, # This will be the second sequence\n",
    "        max_length=max_length,\n",
    "        truncation=True, # Need to consider whitch option is better\n",
    "        padding=False # DataCollator will handle dynamic padding\n",
    "    )\n",
    "    \n",
    "    # Add labels\n",
    "    tokenized_inputs[\"labels\"] = examples[\"labels\"]\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "print(\"All functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd10805",
   "metadata": {},
   "source": [
    "### Step 1. Load All Calibrated Models and Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "model_load_and_predict",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at ../models/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Candidate 1: DeBERTa + LoRA ---\n",
      "C1 model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 192.42 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1 test predictions successfully calibrated.\n",
      "\n",
      "--- Candidate 2: PLM + XGBoost ---\n",
      "try: ../models/e5-base-v2\n",
      "loaded model from: ../models/e5-base-v2\n",
      "C2 test features (X_test_c2) generated.\n",
      "C2 test predictions successfully generated and calibrated.\n",
      "\n",
      "--- Candidate 3: All Features + MLP ---\n",
      "try: ../models/e5-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.conda/envs/jun_cls2/lib/python3.10/site-packages/xgboost/core.py:729: UserWarning: [15:25:54] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model from: ../models/e5-base-v2\n",
      "C3 all test features (X_test_c3) generated.\n",
      "C3 features scaled.\n",
      "C3 test predictions successfully generated and calibrated.\n"
     ]
    }
   ],
   "source": [
    "### C1: DeBERTa + LoRA (Calibrated) ###\n",
    "MODEL_NAME_C1 = \"deberta-v3-base\"\n",
    "LORA_ADAPTER_DIR = f\"{BASE_DIR}/lora_adapter_{MODEL_NAME_C1}\"\n",
    "C1_CALIBRATOR_PATH = f\"{BASE_DIR}/candidate_1_calibrators.pkl\"\n",
    "max_length = 512\n",
    "\n",
    "print(\"\\n--- Candidate 1: DeBERTa + LoRA ---\")\n",
    "# 1. Load Base Model and Peft Adapter\n",
    "try:\n",
    "    base_model_path = f\"{BASE_DIR}/{MODEL_NAME_C1}\"\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_path,\n",
    "        num_labels=3,\n",
    "        local_files_only=True\n",
    "    )\n",
    "    peft_model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_DIR)\n",
    "    peft_model.to(device)\n",
    "    peft_model.eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LORA_ADAPTER_DIR, local_files_only=True)\n",
    "    print(\"C1 model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading C1 model: {e}\")\n",
    "    peft_model = None\n",
    "\n",
    "if peft_model:\n",
    "    # 2. Tokenize Test Data (using C1's preprocessing logic)\n",
    "    def preprocess_test_function(examples):\n",
    "        response_pair = [f\"A: {a} {tokenizer.sep_token} B: {b}\" for a, b in zip(examples['response_a'], examples['response_b'])]\n",
    "        return tokenizer(\n",
    "            examples['prompt'], response_pair, max_length=max_length, truncation=True, padding='max_length'\n",
    "        )\n",
    "    test_dataset = Dataset.from_pandas(test)\n",
    "    tokenized_test_dataset = test_dataset.map(preprocess_test_function, batched=True, remove_columns=test.columns.tolist())\n",
    "    tokenized_test_dataset.set_format(\"torch\")\n",
    "    \n",
    "    # 3. Predict Uncalibrated Probabilities\n",
    "    from torch.utils.data import DataLoader\n",
    "    dl = DataLoader(tokenized_test_dataset, batch_size=8)\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dl:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = peft_model(**batch)\n",
    "            all_logits.append(outputs.logits.cpu().numpy())\n",
    "    test_logits = np.vstack(all_logits)\n",
    "    test_probs_uncalibrated = softmax(test_logits, axis=1)\n",
    "    \n",
    "    # 4. Load Calibrator and Calibrate\n",
    "    try:\n",
    "        calibrators = joblib.load(C1_CALIBRATOR_PATH)\n",
    "        test_probs_calibrated_c1 = np.zeros_like(test_probs_uncalibrated)\n",
    "        for class_idx in range(3):\n",
    "            test_probs_calibrated_c1[:, class_idx] = calibrators[class_idx].predict(test_probs_uncalibrated[:, class_idx])\n",
    "        \n",
    "        # Normalize probabilities to sum to 1\n",
    "        row_sums = test_probs_calibrated_c1.sum(axis=1, keepdims=True)\n",
    "        test_probs_calibrated_c1 = test_probs_calibrated_c1 / np.clip(row_sums, 1e-15, None)\n",
    "        print(\"C1 test predictions successfully calibrated.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading or applying C1 calibrator: {e}. Using uncalibrated.\")\n",
    "        test_probs_calibrated_c1 = test_probs_uncalibrated\n",
    "else:\n",
    "    test_probs_calibrated_c1 = np.zeros((len(test), 3)) # Placeholder for missing model\n",
    "    print(\"C1 skipped due to loading error.\")\n",
    "\n",
    "\n",
    "### C2: PLM + XGBoost (Calibrated) ###\n",
    "MODEL_NAME_C2 = \"e5-base-v2\"\n",
    "C2_MODEL_PATH = f\"{BASE_DIR}/{MODEL_NAME_C2}\"\n",
    "GBM_CHOICE = \"XGBOOST\" # Assuming XGBOOST was the choice\n",
    "C2_CALIBRATED_MODEL_PATH = f\"{BASE_DIR}/candidate_2_{GBM_CHOICE}_{MODEL_NAME_C2}_CALIBRATED.pkl\"\n",
    "\n",
    "print(\"\\n--- Candidate 2: PLM + XGBoost ---\")\n",
    "test_pred_c2_calibrated = None\n",
    "\n",
    "try:\n",
    "    # 1. Load Embedding Model and Generate Test Features (X_test_c2)\n",
    "    sbert_model, _ = load_model([C2_MODEL_PATH], idx=0, device=device)\n",
    "    prompt_emb_te = encode_texts(sbert_model, test[\"prompt\"])\n",
    "    a_emb_te = encode_texts(sbert_model, test[\"response_a\"])\n",
    "    b_emb_te = encode_texts(sbert_model, test[\"response_b\"])\n",
    "    X_test_c2 = build_feat(prompt_emb_te, a_emb_te, b_emb_te)\n",
    "    del sbert_model, prompt_emb_te, a_emb_te, b_emb_te\n",
    "    print(\"C2 test features (X_test_c2) generated.\")\n",
    "    \n",
    "    # 2. Load Final Calibrated Model and Predict\n",
    "    calibrated_final_c2 = joblib.load(C2_CALIBRATED_MODEL_PATH)\n",
    "    test_pred_c2_calibrated = calibrated_final_c2.predict_proba(X_test_c2)\n",
    "    print(\"C2 test predictions successfully generated and calibrated.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR in C2 processing: {e}\")\n",
    "    X_test_c2 = None\n",
    "    test_pred_c2_calibrated = np.zeros((len(test), 3)) # Placeholder\n",
    "\n",
    "\n",
    "### C3: All Features + MLP (Calibrated) ###\n",
    "MODEL_NAME_C3 = \"e5-base-v2\"\n",
    "C3_MODEL_PATH = f\"{BASE_DIR}/{MODEL_NAME_C3}\"\n",
    "C3_CALIBRATED_MODEL_PATH = f\"{BASE_DIR}/candidate_3_MLP_CALIBRATED.pkl\"\n",
    "C3_FINAL_SCALER_PATH = f\"{BASE_DIR}/candidate_3_scaler_final.pkl\"\n",
    "\n",
    "print(\"\\n--- Candidate 3: All Features + MLP ---\")\n",
    "test_pred_c3_calibrated = None\n",
    "\n",
    "try:\n",
    "    # 1. Load Embedding Model and Generate Embedding Features\n",
    "    sbert_model, _ = load_model([C3_MODEL_PATH], idx=0, device=device)\n",
    "    prompt_emb_te = encode_texts(sbert_model, test[\"prompt\"])\n",
    "    a_emb_te = encode_texts(sbert_model, test[\"response_a\"])\n",
    "    b_emb_te = encode_texts(sbert_model, test[\"response_b\"])\n",
    "    X_emb_test_c3 = build_feat(prompt_emb_te, a_emb_te, b_emb_te)\n",
    "    del sbert_model, prompt_emb_te, a_emb_te, b_emb_te\n",
    "    \n",
    "    # 2. Generate Lexical Features\n",
    "    X_test_lex_strong = build_strong_lexical_features(test)\n",
    "    \n",
    "    # 3. Combine and Clean Features (X_test_c3)\n",
    "    X_test_lex_strong = X_test_lex_strong.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    X_emb_test_c3_safe = pd.DataFrame(X_emb_test_c3).fillna(0).replace([np.inf, -np.inf], 0).values\n",
    "    X_test_c3 = np.hstack([X_test_lex_strong, X_emb_test_c3_safe])\n",
    "    print(\"C3 all test features (X_test_c3) generated.\")\n",
    "    \n",
    "    # 4. Load Scaler, Scale Features\n",
    "    scaler_final_c3 = joblib.load(C3_FINAL_SCALER_PATH)\n",
    "    X_test_scaled_c3 = scaler_final_c3.transform(X_test_c3)\n",
    "    print(\"C3 features scaled.\")\n",
    "    \n",
    "    # 5. Load Final Calibrated Model and Predict\n",
    "    calibrated_final_c3 = joblib.load(C3_CALIBRATED_MODEL_PATH)\n",
    "    test_pred_c3_calibrated = calibrated_final_c3.predict_proba(X_test_scaled_c3)\n",
    "    print(\"C3 test predictions successfully generated and calibrated.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR in C3 processing: {e}\")\n",
    "    test_pred_c3_calibrated = np.zeros((len(test), 3)) # Placeholder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d73199f",
   "metadata": {},
   "source": [
    "### Step 2. Final Ensemble: Stacked Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ensemble_train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ensemble Model (Stacked Generalization) Training ---\n",
      "Stacked meta-learner loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"\\n--- Ensemble Model (Stacked Generalization) Training ---\")\n",
    "ENSEMBLE_MODEL_PATH = f\"{BASE_DIR}/meta_learner_stacked.pkl\"\n",
    "\n",
    "# 1. Load Calibrated Validation Predictions (from 01_local_training artifacts)\n",
    "# Note: The ensemble logic relies on having the original validation predictions (val_pred_c1, val_pred_c2, val_pred_c3) \n",
    "#       and the corresponding true labels (y_val_ensemble, y_va_c2_ens, y_va_c3_ens) that were used to find the best strategy.\n",
    "\n",
    "# We will assume the best strategy was 'Stacked_Generalization' based on the result printout.\n",
    "# To train the final meta-learner, we use the full training set predictions from each candidate model.\n",
    "\n",
    "try:\n",
    "    # Load FULL TRAIN SET predictions for C1, C2, C3\n",
    "    full_train_pred_c1 = joblib.load(f'{BASE_DIR}/full_train_pred_c1.pkl')\n",
    "    full_train_pred_c2 = joblib.load(f'{BASE_DIR}/full_train_pred_c2.pkl')\n",
    "    full_train_pred_c3 = joblib.load(f'{BASE_DIR}/full_train_pred_c3.pkl')\n",
    "    \n",
    "    # 2. Build Meta-Features using FULL TRAIN SET predictions\n",
    "    meta_features = np.hstack([full_train_pred_c1, full_train_pred_c2, full_train_pred_c3])\n",
    "    y_full = y # Full labels\n",
    "\n",
    "    # 3. Final Meta-Learner Training on ALL data (split for internal train/test is no longer needed here, but for safety...\n",
    "    # We'll re-run the final fit from 01_local_training (assuming OOF/2nd-level data was saved in the notebook)\n",
    "    # NOTE: Since the full OOF predictions aren't available, we resort to loading the saved meta-learner model.\n",
    "    \n",
    "    meta_learner = joblib.load(ENSEMBLE_MODEL_PATH) # Assume the fully-trained meta-learner was saved.\n",
    "    final_ensemble_method = 'stacked'\n",
    "    print(\"Stacked meta-learner loaded successfully.\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    # If meta-learner was NOT saved in 01_local_training, we must train it here (using the full set is technically train leakage!)\n",
    "    # Best practice is to save the OOF/2nd-level predictions and the meta-learner trained on them.\n",
    "    print(f\"ERROR: {e}\")\n",
    "    print(\"WARNING: Meta-learner not found. Skipping stacked ensemble.\")\n",
    "    final_ensemble_method = None\n",
    "\n",
    "# Save decision variables for next cell (copied from original notebook's final decision)\n",
    "if final_ensemble_method == 'stacked':\n",
    "    final_ensemble_weights, final_ensemble_method = None, 'stacked'\n",
    "else:\n",
    "    # Fallback to Simple Average if Stacked fails to load (using the known failure in the original notebook)\n",
    "    print(\"Falling back to Simple Average.\")\n",
    "    final_ensemble_weights, final_ensemble_method = np.array([1/3, 1/3, 1/3]), 'simple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ensemble_predict",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATING FINAL ENSEMBLE PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "Using ensemble method: STACKED\n",
      "\n",
      "Final predictions shape: (3, 3)\n",
      "Probability sum check (first 3): [1. 1. 1.]\n",
      "Creating submission file: submission.csv...\n",
      "Successfully saved and verified: submission.csv (Shape: (3, 4))\n",
      "\n",
      "================================================================================\n",
      "FINAL ENSEMBLE SUBMISSION CREATED\n",
      "File: submission.csv\n",
      "Method: STACKED\n",
      "Expected Validation LogLoss: ~1.071608\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING FINAL ENSEMBLE PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if final_ensemble_method is not None:\n",
    "    # Use the calibrated test predictions from Step 1\n",
    "    test_pred_c1 = test_probs_calibrated_c1\n",
    "    test_pred_c2 = test_pred_c2_calibrated\n",
    "    test_pred_c3 = test_pred_c3_calibrated\n",
    "    \n",
    "    # Safety check on shapes\n",
    "    if test_pred_c1.shape[0] != len(test) or test_pred_c2.shape[0] != len(test) or test_pred_c3.shape[0] != len(test):\n",
    "        print(\"ERROR: Test prediction shapes mismatch. Cannot ensemble.\")\n",
    "        final_ensemble_method = None\n",
    "\n",
    "if final_ensemble_method is not None:\n",
    "    print(f\"\\nUsing ensemble method: {final_ensemble_method.upper()}\")\n",
    "    \n",
    "    if final_ensemble_method == 'simple':\n",
    "        final_test_pred = (test_pred_c1 + test_pred_c2 + test_pred_c3) / 3.0\n",
    "    elif final_ensemble_method in ['optimal', 'weighted']:\n",
    "        # NOTE: Optimal/Weighted weights are not loaded here, so this case is unlikely to run correctly.\n",
    "        # The original notebook defaulted to simple average if the best method couldn't be run.\n",
    "        final_test_pred = (\n",
    "            final_ensemble_weights[0] * test_pred_c1 +\n",
    "            final_ensemble_weights[1] * test_pred_c2 +\n",
    "            final_ensemble_weights[2] * test_pred_c3\n",
    "        )\n",
    "    elif final_ensemble_method == 'stacked':\n",
    "        test_meta_features = np.hstack([test_pred_c1, test_pred_c2, test_pred_c3])\n",
    "        # Load and use the meta-learner object loaded in the previous cell\n",
    "        final_test_pred = meta_learner.predict_proba(test_meta_features)\n",
    "    elif final_ensemble_method == 'rank':\n",
    "        # Rank averaging logic from 02_kaggle_inference\n",
    "        def probs_to_ranks_test(probs):\n",
    "            ranks = np.zeros_like(probs, dtype=int)\n",
    "            for i in range(probs.shape[0]):\n",
    "                ranks[i] = np.argsort(np.argsort(-probs[i]))\n",
    "            return ranks\n",
    "        ranks_test_c1 = probs_to_ranks_test(test_pred_c1)\n",
    "        ranks_test_c2 = probs_to_ranks_test(test_pred_c2)\n",
    "        ranks_test_c3 = probs_to_ranks_test(test_pred_c3)\n",
    "        avg_ranks_test = (ranks_test_c1 + ranks_test_c2 + ranks_test_c3) / 3.0\n",
    "        num_classes_test = test_pred_c1.shape[1]\n",
    "        final_test_pred = np.zeros_like(test_pred_c1, dtype=float)\n",
    "        for i in range(avg_ranks_test.shape[0]):\n",
    "            scores = float(num_classes_test) - avg_ranks_test[i]\n",
    "            scores = np.exp(scores)\n",
    "            final_test_pred[i] = scores / scores.sum()\n",
    "\n",
    "    # Safety normalization\n",
    "    row_sums = final_test_pred.sum(axis=1, keepdims=True)\n",
    "    final_test_pred = final_test_pred / np.clip(row_sums, 1e-15, None)\n",
    "\n",
    "    print(f\"\\nFinal predictions shape: {final_test_pred.shape}\")\n",
    "    print(f\"Probability sum check (first 3): {final_test_pred[:3].sum(axis=1)}\")\n",
    "\n",
    "    final_filename = f\"submission.csv\"\n",
    "    create_and_save_submission(\n",
    "        predictions=final_test_pred,\n",
    "        filename=final_filename,\n",
    "        test_df=test,\n",
    "        sample_df=sample\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL ENSEMBLE SUBMISSION CREATED\")\n",
    "    print(f\"File: {final_filename}\")\n",
    "    print(f\"Method: {final_ensemble_method.upper()}\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\nERROR: Ensemble method not determined or prediction failed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jun_cls2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
